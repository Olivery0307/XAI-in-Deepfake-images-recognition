{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"711124bc988c483182e547b57848b7dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9c22d583bb8412aa782e68c0c920c86","IPY_MODEL_628b1e6510cf46338d0e010fe095f2c4","IPY_MODEL_ff990731f4f645e2a8258dcecc41401a"],"layout":"IPY_MODEL_b20e68a1589942b4820b320bfd0a7b58"}},"a9c22d583bb8412aa782e68c0c920c86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_122561f392d14e3cafc6debb3187e24f","placeholder":"​","style":"IPY_MODEL_bfce42766f244ecd957c91c5ac56fd21","value":"preprocessor_config.json: 100%"}},"628b1e6510cf46338d0e010fe095f2c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb2707c882e94a97b32088ae4d492133","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f572c2a50ae64510bbf92c40ace30fc2","value":160}},"ff990731f4f645e2a8258dcecc41401a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9dd59964a914d649291782d647463c3","placeholder":"​","style":"IPY_MODEL_ded7bcb289394a17a9ccb1f40e17fa0c","value":" 160/160 [00:00&lt;00:00, 13.2kB/s]"}},"b20e68a1589942b4820b320bfd0a7b58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"122561f392d14e3cafc6debb3187e24f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfce42766f244ecd957c91c5ac56fd21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb2707c882e94a97b32088ae4d492133":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f572c2a50ae64510bbf92c40ace30fc2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9dd59964a914d649291782d647463c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ded7bcb289394a17a9ccb1f40e17fa0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13728437,"sourceType":"datasetVersion","datasetId":8734376},{"sourceId":13736632,"sourceType":"datasetVersion","datasetId":8706215}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set up","metadata":{"id":"BO5FnImoFEWY"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom transformers import ViTForImageClassification, TrainingArguments, Trainer, ViTImageProcessor, EarlyStoppingCallback, AutoImageProcessor, SwinForImageClassification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom PIL import Image, ImageFilter\nimport os\nimport io\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport sys\nimport random\nfrom collections import defaultdict\nimport glob","metadata":{"id":"mRdgDUABFyeF","executionInfo":{"status":"ok","timestamp":1762829608417,"user_tz":300,"elapsed":63210,"user":{"displayName":"LIANG-JIE CHIU","userId":"03072225676804136983"}},"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:40:23.627807Z","iopub.execute_input":"2025-11-15T00:40:23.628076Z","iopub.status.idle":"2025-11-15T00:40:58.717921Z","shell.execute_reply.started":"2025-11-15T00:40:23.628046Z","shell.execute_reply":"2025-11-15T00:40:58.717327Z"}},"outputs":[{"name":"stderr","text":"2025-11-15 00:40:37.902389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763167238.100130      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763167238.155286      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{"id":"RbE3cpAmFnZ5"}},{"cell_type":"code","source":"class KaggleImageDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset to load images directly from the local file system\n    for use with HuggingFace Trainer, with optional augmentations.\n    \"\"\"\n    def __init__(self, file_paths, labels, processor, is_train=False):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.processor = processor\n        self.is_train = is_train  \n        \n        # Define augmentations for training\n        if self.is_train:\n            self.augmentations = transforms.Compose([\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.RandomApply([\n                    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n                ], p=0.3),\n                transforms.RandomApply([\n                    transforms.GaussianBlur(kernel_size=3)\n                ], p=0.3),\n                transforms.RandomRotation(degrees=10),\n            ])\n        else:\n            self.augmentations = None\n\n    def __len__(self):\n        \"\"\"Returns the total number of samples.\"\"\"\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Fetches the image from local path, applies augmentations and processor, \n        and returns the sample in HuggingFace format.\n        \"\"\"\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        # Get the local path and label\n        local_path = self.file_paths[idx]\n        label = self.labels[idx]\n\n        try:\n            # Open the file from local path\n            image = Image.open(local_path).convert('RGB')\n\n            # Apply augmentations BEFORE the processor (only for training)\n            if self.augmentations is not None:\n                image = self.augmentations(image)\n\n            # Use the ViT processor (handles resizing and normalization)\n            processed = self.processor(images=image, return_tensors=\"pt\")\n\n            # Extract the pixel values and remove the batch dimension\n            pixel_values = processed['pixel_values'].squeeze(0)\n\n            # Return in HuggingFace format\n            return {\n                'pixel_values': pixel_values,\n                'labels': torch.tensor(label, dtype=torch.long)\n            }\n\n        except Exception as e:\n            print(f\"Error loading image {local_path}: {e}\")\n            # Return a dummy sample if loading fails\n            dummy_image = Image.new('RGB', (224, 224), color='black')\n            processed = self.processor(images=dummy_image, return_tensors=\"pt\")\n            return {\n                'pixel_values': processed['pixel_values'].squeeze(0),\n                'labels': torch.tensor(0, dtype=torch.long)\n            }","metadata":{"id":"1aD4MTCrFqqu","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:41:33.348357Z","iopub.execute_input":"2025-11-15T00:41:33.349081Z","iopub.status.idle":"2025-11-15T00:41:33.361136Z","shell.execute_reply.started":"2025-11-15T00:41:33.349045Z","shell.execute_reply":"2025-11-15T00:41:33.360296Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def get_data_mixed_structure(video_real_paths, video_fake_paths, \n                              image_real_paths, image_fake_paths, \n                              model_name, random_seed):\n    \"\"\"\n    Scans local directories and handles two types of datasets:\n    1. Video-based: Split BY FOLDER to prevent frame leakage\n    2. Image-based: Split BY IMAGE (no folder structure)\n    \n    Args:\n        video_real_paths: List of paths to real video folders (e.g., Celeb-real, YouTube-real)\n        video_fake_paths: List of paths to fake video folders (e.g., Celeb-synthesis)\n        image_real_paths: List of paths to real image folders (e.g., FFHQ-real-v2)\n        image_fake_paths: List of paths to fake image folders (e.g., StableDiffusion-fake-v2, stylegan-6000)\n        model_name: HuggingFace model name for processor\n        random_seed: Random seed for reproducibility\n    \"\"\"\n    \n    patterns_to_check = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n    \n    # ========================================\n    # PART 1: Handle VIDEO-BASED datasets (split by folder)\n    # ========================================\n    real_video_folders = defaultdict(list)\n    fake_video_folders = defaultdict(list)\n    \n    # Get REAL video folders\n    for path in video_real_paths:\n        for ext in patterns_to_check:\n            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n            for file in files:\n                parent_folder = os.path.dirname(file)\n                real_video_folders[parent_folder].append(file)\n        \n        print(f\"Found {len(real_video_folders)} REAL video folders in {path}\")\n        total_files = sum(len(files) for files in real_video_folders.values())\n        print(f\"  Total REAL video frames: {total_files}\")\n    \n    # Get FAKE video folders\n    for path in video_fake_paths:\n        for ext in patterns_to_check:\n            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n            for file in files:\n                parent_folder = os.path.dirname(file)\n                fake_video_folders[parent_folder].append(file)\n        \n        print(f\"Found {len(fake_video_folders)} FAKE video folders in {path}\")\n        total_files = sum(len(files) for files in fake_video_folders.values())\n        print(f\"  Total FAKE video frames: {total_files}\")\n    \n    # Split video folders (70/15/15)\n    train_real_video_folders, val_real_video_folders, test_real_video_folders = [], [], []\n    train_fake_video_folders, val_fake_video_folders, test_fake_video_folders = [], [], []\n    \n    if len(real_video_folders) > 0:\n        real_folder_names = list(real_video_folders.keys())\n        train_real_video_folders, temp_real = train_test_split(\n            real_folder_names, test_size=0.3, random_state=random_seed\n        )\n        val_real_video_folders, test_real_video_folders = train_test_split(\n            temp_real, test_size=0.5, random_state=random_seed\n        )\n    \n    if len(fake_video_folders) > 0:\n        fake_folder_names = list(fake_video_folders.keys())\n        train_fake_video_folders, temp_fake = train_test_split(\n            fake_folder_names, test_size=0.3, random_state=random_seed\n        )\n        val_fake_video_folders, test_fake_video_folders = train_test_split(\n            temp_fake, test_size=0.5, random_state=random_seed\n        )\n    \n    # ========================================\n    # PART 2: Handle IMAGE-BASED datasets (split by image)\n    # ========================================\n    real_image_files = []\n    fake_image_files = []\n    \n    # Get REAL image files\n    for path in image_real_paths:\n        for ext in patterns_to_check:\n            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n            real_image_files.extend(files)\n        print(f\"Found {len([f for f in real_image_files if path in f])} REAL images in {path}\")\n    \n    print(f\"  Total REAL images: {len(real_image_files)}\")\n    \n    # Get FAKE image files\n    for path in image_fake_paths:\n        for ext in patterns_to_check:\n            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n            fake_image_files.extend(files)\n        print(f\"Found {len([f for f in fake_image_files if path in f])} FAKE images in {path}\")\n    \n    print(f\"  Total FAKE images: {len(fake_image_files)}\")\n    \n    # Split image files (70/15/15)\n    train_real_images, val_real_images, test_real_images = [], [], []\n    train_fake_images, val_fake_images, test_fake_images = [], [], []\n    \n    if len(real_image_files) > 0:\n        train_real_images, temp_real = train_test_split(\n            real_image_files, test_size=0.3, random_state=random_seed\n        )\n        val_real_images, test_real_images = train_test_split(\n            temp_real, test_size=0.5, random_state=random_seed\n        )\n    \n    if len(fake_image_files) > 0:\n        train_fake_images, temp_fake = train_test_split(\n            fake_image_files, test_size=0.3, random_state=random_seed\n        )\n        val_fake_images, test_fake_images = train_test_split(\n            temp_fake, test_size=0.5, random_state=random_seed\n        )\n    \n    # ========================================\n    # PART 3: Combine video-based and image-based data\n    # ========================================\n    train_files, train_labels = [], []\n    val_files, val_labels = [], []\n    test_files, test_labels = [], []\n    \n    # Add REAL VIDEO frames to splits\n    for folder in train_real_video_folders:\n        train_files.extend(real_video_folders[folder])\n        train_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n    \n    for folder in val_real_video_folders:\n        val_files.extend(real_video_folders[folder])\n        val_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n    \n    for folder in test_real_video_folders:\n        test_files.extend(real_video_folders[folder])\n        test_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n    \n    # Add FAKE VIDEO frames to splits\n    for folder in train_fake_video_folders:\n        train_files.extend(fake_video_folders[folder])\n        train_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n    \n    for folder in val_fake_video_folders:\n        val_files.extend(fake_video_folders[folder])\n        val_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n    \n    for folder in test_fake_video_folders:\n        test_files.extend(fake_video_folders[folder])\n        test_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n    \n    # Add REAL IMAGES to splits\n    train_files.extend(train_real_images)\n    train_labels.extend([LABEL_REAL] * len(train_real_images))\n    \n    val_files.extend(val_real_images)\n    val_labels.extend([LABEL_REAL] * len(val_real_images))\n    \n    test_files.extend(test_real_images)\n    test_labels.extend([LABEL_REAL] * len(test_real_images))\n    \n    # Add FAKE IMAGES to splits\n    train_files.extend(train_fake_images)\n    train_labels.extend([LABEL_FAKE] * len(train_fake_images))\n    \n    val_files.extend(val_fake_images)\n    val_labels.extend([LABEL_FAKE] * len(val_fake_images))\n    \n    test_files.extend(test_fake_images)\n    test_labels.extend([LABEL_FAKE] * len(test_fake_images))\n    \n    # ========================================\n    # PART 4: Print detailed statistics\n    # ========================================\n    print(\"\\n\" + \"=\"*70)\n    print(\"MIXED DATASET STATISTICS (Video-based + Image-based)\")\n    print(\"=\"*70)\n    \n    print(\"\\n--- VIDEO-BASED DATA (split by folder) ---\")\n    if len(real_video_folders) > 0:\n        print(f\"Real video folders: {len(real_video_folders)} total\")\n        print(f\"  Train: {len(train_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in train_real_video_folders)} frames\")\n        print(f\"  Val:   {len(val_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in val_real_video_folders)} frames\")\n        print(f\"  Test:  {len(test_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in test_real_video_folders)} frames\")\n    else:\n        print(\"No real video data\")\n    \n    if len(fake_video_folders) > 0:\n        print(f\"\\nFake video folders: {len(fake_video_folders)} total\")\n        print(f\"  Train: {len(train_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in train_fake_video_folders)} frames\")\n        print(f\"  Val:   {len(val_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in val_fake_video_folders)} frames\")\n        print(f\"  Test:  {len(test_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in test_fake_video_folders)} frames\")\n    else:\n        print(\"No fake video data\")\n    \n    print(\"\\n--- IMAGE-BASED DATA (split by image) ---\")\n    if len(real_image_files) > 0:\n        print(f\"Real images: {len(real_image_files)} total\")\n        print(f\"  Train: {len(train_real_images)} images\")\n        print(f\"  Val:   {len(val_real_images)} images\")\n        print(f\"  Test:  {len(test_real_images)} images\")\n    else:\n        print(\"No real image data\")\n    \n    if len(fake_image_files) > 0:\n        print(f\"\\nFake images: {len(fake_image_files)} total\")\n        print(f\"  Train: {len(train_fake_images)} images\")\n        print(f\"  Val:   {len(val_fake_images)} images\")\n        print(f\"  Test:  {len(test_fake_images)} images\")\n    else:\n        print(\"No fake image data\")\n    \n    print(\"\\n--- COMBINED TOTALS ---\")\n    print(f\"Train: {len(train_files)} total ({train_labels.count(LABEL_REAL)} real, {train_labels.count(LABEL_FAKE)} fake)\")\n    print(f\"Val:   {len(val_files)} total ({val_labels.count(LABEL_REAL)} real, {val_labels.count(LABEL_FAKE)} fake)\")\n    print(f\"Test:  {len(test_files)} total ({test_labels.count(LABEL_REAL)} real, {test_labels.count(LABEL_FAKE)} fake)\")\n    print(f\"\\nGrand Total: {len(train_files) + len(val_files) + len(test_files)} images\")\n    print(\"=\"*70)\n    \n    # ========================================\n    # PART 5: Create datasets\n    # ========================================\n    processor = AutoImageProcessor.from_pretrained(model_name, use_fast = True)\n    \n    train_dataset = KaggleImageDataset(\n        file_paths=train_files,\n        labels=train_labels,\n        processor=processor,\n        is_train=True\n    )\n    val_dataset = KaggleImageDataset(\n        file_paths=val_files,\n        labels=val_labels,\n        processor=processor,\n        is_train=False\n    )\n    test_dataset = KaggleImageDataset(\n        file_paths=test_files,\n        labels=test_labels,\n        processor=processor,\n        is_train=False\n    )\n    \n    return train_dataset, val_dataset, test_dataset, processor","metadata":{"id":"socv-cDOJv19","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:41:47.942676Z","iopub.execute_input":"2025-11-15T00:41:47.942957Z","iopub.status.idle":"2025-11-15T00:41:47.964456Z","shell.execute_reply.started":"2025-11-15T00:41:47.942937Z","shell.execute_reply":"2025-11-15T00:41:47.963663Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    \n    # Already converted to predictions, not logits\n    if isinstance(predictions, torch.Tensor):\n        predictions = predictions.cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.cpu().numpy()\n    \n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='binary'\n    )\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }","metadata":{"id":"TORZeyN6Qpx_","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:41:49.127629Z","iopub.execute_input":"2025-11-15T00:41:49.128205Z","iopub.status.idle":"2025-11-15T00:41:49.132918Z","shell.execute_reply.started":"2025-11-15T00:41:49.128179Z","shell.execute_reply":"2025-11-15T00:41:49.132078Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MemoryEfficientTrainer(Trainer):\n    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n        \"\"\"\n        Override to return predictions instead of full logits\n        \"\"\"\n        inputs = self._prepare_inputs(inputs)\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n            loss = outputs.loss\n            logits = outputs.logits\n        \n        # Return predictions instead of logits to save memory\n        if prediction_loss_only:\n            return (loss, None, None)\n        \n        # Convert to predictions immediately\n        preds = torch.argmax(logits, dim=-1)\n        labels = inputs.get(\"labels\")\n        \n        return (loss, preds, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:41:58.520174Z","iopub.execute_input":"2025-11-15T00:41:58.520994Z","iopub.status.idle":"2025-11-15T00:41:58.526796Z","shell.execute_reply.started":"2025-11-15T00:41:58.520960Z","shell.execute_reply":"2025-11-15T00:41:58.526001Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def attetion_rollout(attentions, discard_ratio=0.9):\n    \"\"\"\n    Compute attention rollout from all transformer layers.\n    Args:\n        attentions: tuple of attention tensors from each layer\n        discard_ratio: percentage of lowest attention values to discard\n    Returns:\n        Attention map for the [CLS] token\n    \"\"\"\n    # Get device from first attention tensor\n    device = attentions[0].device\n    \n    # Create identity matrix on the same device\n    result = torch.eye(attentions[0].size(-1)).to(device)\n    \n    for attention in attentions:\n        # Average across all heads\n        attention_heads_fused = attention.mean(dim=1)\n        attention_heads_fused = attention_heads_fused[0]\n        \n        # Drop the lowest attentions\n        flat = attention_heads_fused.view(-1)\n        _, indices = flat.topk(k=int(flat.size(-1) * discard_ratio), largest=False)\n        flat[indices] = 0\n        \n        # Normalize\n        I = torch.eye(attention_heads_fused.size(-1)).to(device)  # Fix: add .to(device)\n        a = (attention_heads_fused + 1.0 * I) / 2\n        a = a / a.sum(dim=-1, keepdim=True)\n        result = torch.matmul(a, result)\n    \n    mask = result[0, 1:]\n    return mask","metadata":{"id":"MZdo0m85x9UE","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:42:00.568112Z","iopub.execute_input":"2025-11-15T00:42:00.568406Z","iopub.status.idle":"2025-11-15T00:42:00.574631Z","shell.execute_reply.started":"2025-11-15T00:42:00.568385Z","shell.execute_reply":"2025-11-15T00:42:00.573787Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def visualize_attention(model, image_path, processor, true_label=None):\n    \"\"\"\n    Visualize attention rollout for a single image.\n    \n    Args:\n        model: ViT model with output_attentions=True\n        image_path: Path to local image file\n        processor: ViTImageProcessor for preprocessing\n        true_label: Optional true label (0 for FAKE, 1 for REAL, or string)\n    \"\"\"\n    # Load image from local path\n    image = Image.open(image_path).convert('RGB')\n    \n    # Process image\n    inputs = processor(images=image, return_tensors=\"pt\")\n    \n    # Move inputs to same device as model\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # Get model outputs with attentions\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs, output_attentions=True)\n    \n    # Get attention weights\n    attentions = outputs.attentions  # tuple of (num_layers) tensors\n    \n    # Compute attention rollout\n    mask = attetion_rollout(attentions)\n    \n    # Reshape mask to image dimensions\n    num_patches = int(mask.shape[0] ** 0.5)\n    mask = mask.reshape(num_patches, num_patches).cpu().numpy()\n    \n    # Resize to original image size\n    mask = Image.fromarray((mask * 255).astype(np.uint8)).resize(\n        image.size, resample=Image.BILINEAR\n    )\n    mask = np.array(mask) / 255.0\n    \n    # Visualize\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Original image\n    axes[0].imshow(image)\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n    \n    # Attention heatmap\n    axes[1].imshow(mask, cmap='jet')\n    axes[1].set_title('Attention Rollout')\n    axes[1].axis('off')\n    \n    # Overlay\n    axes[2].imshow(image)\n    axes[2].imshow(mask, cmap='jet', alpha=0.5)\n    axes[2].set_title('Overlay')\n    axes[2].axis('off')\n    \n    # Get prediction\n    prediction = outputs.logits.argmax(-1).item()\n    pred_label = model.config.id2label[prediction]\n    prob = torch.softmax(outputs.logits, dim=-1)[0][prediction].item()\n    \n    # Build title with prediction and true label\n    title_parts = [f'Prediction: {pred_label} ({prob:.2%})']\n    \n    if true_label is not None:\n        # Convert true_label to string if it's numeric\n        if isinstance(true_label, (int, np.integer)):\n            true_label_str = model.config.id2label[true_label]\n        else:\n            true_label_str = true_label\n        \n        # Check if prediction is correct\n        is_correct = (pred_label == true_label_str)\n        correctness = \"✓\" if is_correct else \"✗\"\n        \n        title_parts.append(f'True Label: {true_label_str} {correctness}')\n    \n    fig.suptitle(' | '.join(title_parts), fontsize=16)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return mask","metadata":{"id":"SW-RbGL81XRQ","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:42:03.662709Z","iopub.execute_input":"2025-11-15T00:42:03.663343Z","iopub.status.idle":"2025-11-15T00:42:03.672822Z","shell.execute_reply.started":"2025-11-15T00:42:03.663317Z","shell.execute_reply":"2025-11-15T00:42:03.671986Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def seed_everything(seed: int):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"id":"LsCKdndrS27Q","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:42:06.653778Z","iopub.execute_input":"2025-11-15T00:42:06.654112Z","iopub.status.idle":"2025-11-15T00:42:06.659154Z","shell.execute_reply.started":"2025-11-15T00:42:06.654086Z","shell.execute_reply":"2025-11-15T00:42:06.658243Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Define Parameters and Input\nvideo_real_paths = [\n    \"/kaggle/input/deepfake-images/Celeb-DF/data/Celeb-real\",\n    \"/kaggle/input/deepfake-images/Celeb-DF/data/YouTube-real\"\n]\nvideo_fake_paths = [\n    \"/kaggle/input/deepfake-images/Celeb-DF/data/Celeb-synthesis\"\n]\n\nimage_real_paths = [\n    \"/kaggle/input/deepfake-images/FFHQ-real-v2/FFHQ-real-v2\"\n]\n\nimage_fake_paths = [\n    \"/kaggle/input/deepfake-images/StableDiffusion-fake-v2/StableDiffusion-fake-v2\",\n    \"/kaggle/input/stylegan-6000/kaggle/working/stylegan_fake_dataset_nvidia\"\n]\n\nLABEL_REAL = 0\nLABEL_FAKE = 1\n\nIMG_SIZE = 224\nBATCH_SIZE = 16\nRANDOM_SEED = 42\nEPOCHS = 10\nLEARNING_RATE = 2e-5\n\nmodel_name = \"google/vit-base-patch16-224\"","metadata":{"id":"cuAo95zcRL5V","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:42:07.470793Z","iopub.execute_input":"2025-11-15T00:42:07.471488Z","iopub.status.idle":"2025-11-15T00:42:07.477084Z","shell.execute_reply.started":"2025-11-15T00:42:07.471453Z","shell.execute_reply":"2025-11-15T00:42:07.476189Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"seed_everything(RANDOM_SEED)","metadata":{"id":"3SyLZKpcS1MK","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:42:09.879200Z","iopub.execute_input":"2025-11-15T00:42:09.879531Z","iopub.status.idle":"2025-11-15T00:42:09.887482Z","shell.execute_reply.started":"2025-11-15T00:42:09.879495Z","shell.execute_reply":"2025-11-15T00:42:09.886756Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_dataset, val_dataset, test_dataset, processor = get_data_mixed_structure(\n    video_real_paths = video_real_paths,\n    video_fake_paths = video_fake_paths,\n    image_real_paths = image_real_paths,\n    image_fake_paths = image_fake_paths,\n    model_name = model_name,\n    random_seed = RANDOM_SEED\n)","metadata":{"id":"vBGXsxCVKdR9","outputId":"2a119f29-d678-46db-e551-584db4dc1f1d","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:42:11.221193Z","iopub.execute_input":"2025-11-15T00:42:11.221471Z","iopub.status.idle":"2025-11-15T00:42:56.449781Z","shell.execute_reply.started":"2025-11-15T00:42:11.221451Z","shell.execute_reply":"2025-11-15T00:42:56.448960Z"}},"outputs":[{"name":"stdout","text":"Found 590 REAL video folders in /kaggle/input/deepfake-images/Celeb-DF/data/Celeb-real\n  Total REAL video frames: 11536\nFound 890 REAL video folders in /kaggle/input/deepfake-images/Celeb-DF/data/YouTube-real\n  Total REAL video frames: 18352\nFound 2361 FAKE video folders in /kaggle/input/deepfake-images/Celeb-DF/data/Celeb-synthesis\n  Total FAKE video frames: 45228\nFound 4500 REAL images in /kaggle/input/deepfake-images/FFHQ-real-v2/FFHQ-real-v2\n  Total REAL images: 4500\nFound 3636 FAKE images in /kaggle/input/deepfake-images/StableDiffusion-fake-v2/StableDiffusion-fake-v2\nFound 6000 FAKE images in /kaggle/input/stylegan-6000/kaggle/working/stylegan_fake_dataset_nvidia\n  Total FAKE images: 9636\n\n======================================================================\nMIXED DATASET STATISTICS (Video-based + Image-based)\n======================================================================\n\n--- VIDEO-BASED DATA (split by folder) ---\nReal video folders: 890 total\n  Train: 623 folders, 12771 frames\n  Val:   133 folders, 2739 frames\n  Test:  134 folders, 2842 frames\n\nFake video folders: 2361 total\n  Train: 1652 folders, 31822 frames\n  Val:   354 folders, 6695 frames\n  Test:  355 folders, 6711 frames\n\n--- IMAGE-BASED DATA (split by image) ---\nReal images: 4500 total\n  Train: 3150 images\n  Val:   675 images\n  Test:  675 images\n\nFake images: 9636 total\n  Train: 6745 images\n  Val:   1445 images\n  Test:  1446 images\n\n--- COMBINED TOTALS ---\nTrain: 54488 total (15921 real, 38567 fake)\nVal:   11554 total (3414 real, 8140 fake)\nTest:  11674 total (3517 real, 8157 fake)\n\nGrand Total: 77716 images\n======================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"204544a03146443f9b583a1c3c51d190"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5886815dfea4c2fac5d78b206df5a17"}},"metadata":{}},{"name":"stderr","text":"Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:43:34.662692Z","iopub.execute_input":"2025-11-15T00:43:34.663052Z","iopub.status.idle":"2025-11-15T00:43:34.667461Z","shell.execute_reply.started":"2025-11-15T00:43:34.662997Z","shell.execute_reply":"2025-11-15T00:43:34.666669Z"}},"outputs":[{"name":"stdout","text":"Training dataset size: 54488\nValidation dataset size: 11554\nTest dataset size: 11674\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"model = ViTForImageClassification.from_pretrained(\n    model_name,\n    num_labels = 2,\n    id2label = {0: \"FAKE\", 1: \"REAL\"},\n    label2id = {\"FAKE\": 0, \"REAL\": 1},\n    ignore_mismatched_sizes = True,\n    output_attentions = True\n)","metadata":{"id":"VMrtMHdRRXSJ","outputId":"c4c59d37-9f29-4901-8ff4-0e1a6815839e","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:44:24.757921Z","iopub.execute_input":"2025-11-15T00:44:24.758872Z","iopub.status.idle":"2025-11-15T00:44:26.956696Z","shell.execute_reply.started":"2025-11-15T00:44:24.758847Z","shell.execute_reply":"2025-11-15T00:44:26.955941Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"861ba86836834242a46f47be2ef98b11"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"for param in model.vit.embeddings.parameters():\n    param.requires_grad = False\n\n# Freeze all encoder layers except the last 2\nnum_layers = len(model.vit.encoder.layer)\nfor i, layer in enumerate(model.vit.encoder.layer):\n    if i < num_layers - 2:  # Freeze all but last 2 layers\n        for param in layer.parameters():\n            param.requires_grad = False\n\nfor param in model.vit.layernorm.parameters():\n    param.requires_grad = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:44:29.755849Z","iopub.execute_input":"2025-11-15T00:44:29.756172Z","iopub.status.idle":"2025-11-15T00:44:29.761904Z","shell.execute_reply.started":"2025-11-15T00:44:29.756149Z","shell.execute_reply":"2025-11-15T00:44:29.761106Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:44:31.509395Z","iopub.execute_input":"2025-11-15T00:44:31.509877Z","iopub.status.idle":"2025-11-15T00:44:31.517213Z","shell.execute_reply.started":"2025-11-15T00:44:31.509844Z","shell.execute_reply":"2025-11-15T00:44:31.516358Z"}},"outputs":[{"name":"stdout","text":"Trainable parameters: 14,178,818 / 85,800,194 (16.53%)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"early_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience = 5,\n    early_stopping_threshold = 0.001\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:44:32.481691Z","iopub.execute_input":"2025-11-15T00:44:32.481971Z","iopub.status.idle":"2025-11-15T00:44:32.485485Z","shell.execute_reply.started":"2025-11-15T00:44:32.481952Z","shell.execute_reply":"2025-11-15T00:44:32.484810Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir = \"./vit-fake-detector_freezed\",\n    per_device_train_batch_size = BATCH_SIZE,\n    per_device_eval_batch_size = 8,\n    num_train_epochs = EPOCHS,\n    eval_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate = LEARNING_RATE,\n    weight_decay = 0.01,\n    warmup_ratio = 0.1,\n    load_best_model_at_end = True,\n    metric_for_best_model = \"f1\",\n    greater_is_better = True,\n    logging_dir = './logs',\n    logging_steps = 100,\n    remove_unused_columns = False,\n    push_to_hub = False,\n    report_to = \"none\",\n    save_total_limit = 2,\n    dataloader_pin_memory = False\n)","metadata":{"id":"CSkCRB30QtLm","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:44:33.756537Z","iopub.execute_input":"2025-11-15T00:44:33.756821Z","iopub.status.idle":"2025-11-15T00:44:33.784715Z","shell.execute_reply.started":"2025-11-15T00:44:33.756802Z","shell.execute_reply":"2025-11-15T00:44:33.784194Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"trainer = MemoryEfficientTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset,\n    compute_metrics = compute_metrics,\n    callbacks = [early_stopping_callback]\n)","metadata":{"id":"39g9QFKGR2Aq","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:44:34.972489Z","iopub.execute_input":"2025-11-15T00:44:34.973046Z","iopub.status.idle":"2025-11-15T00:44:35.252056Z","shell.execute_reply.started":"2025-11-15T00:44:34.973004Z","shell.execute_reply":"2025-11-15T00:44:35.251280Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:44:35.761365Z","iopub.execute_input":"2025-11-15T00:44:35.762148Z","iopub.status.idle":"2025-11-15T00:44:36.127725Z","shell.execute_reply.started":"2025-11-15T00:44:35.762122Z","shell.execute_reply":"2025-11-15T00:44:36.126928Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"120"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"qEsp09tJR75b","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:44:38.848351Z","iopub.execute_input":"2025-11-15T00:44:38.848890Z"}},"outputs":[{"name":"stderr","text":"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1107' max='34060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1107/34060 09:27 < 4:42:06, 1.95 it/s, Epoch 0.32/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"test_results = trainer.evaluate(test_dataset)\nprint(f\"Test results: {test_results}\")","metadata":{"id":"9gj9rE-MR9Y8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = trainer.model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience = 5,\n    early_stopping_threshold = 0.001\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir = \"./vit-fake-detector_unfreezed\",\n    per_device_train_batch_size = BATCH_SIZE,\n    per_device_eval_batch_size = 8,\n    num_train_epochs = EPOCHS,\n    eval_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate = LEARNING_RATE,\n    weight_decay = 0.01,\n    warmup_ratio = 0.1,\n    load_best_model_at_end = True,\n    metric_for_best_model = \"f1\",\n    greater_is_better = True,\n    logging_dir = './logs',\n    logging_steps = 100,\n    remove_unused_columns = False,\n    push_to_hub = False,\n    report_to = \"none\",\n    save_total_limit = 2,\n    dataloader_pin_memory = False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = MemoryEfficientTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset,\n    compute_metrics = compute_metrics,\n    callbacks = [early_stopping_callback]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processor = AutoImageProcessor.from_pretrained(model_name)","metadata":{"id":"h48Tt5pf1vKR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model = trainer.model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n\nfor idx in tmp:\n    image_path = test_dataset.file_paths[idx]\n    true_label = test_dataset.labels[idx]\n    visualize_attention(best_model, image_path, processor, true_label = true_label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n\nfor idx in tmp:\n    image_path = test_dataset.file_paths[idx]\n    true_label = test_dataset.labels[idx]\n    visualize_attention(best_model, image_path, processor, true_label = true_label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n\nfor idx in tmp:\n    image_path = test_dataset.file_paths[idx]\n    true_label = test_dataset.labels[idx]\n    visualize_attention(best_model, image_path, processor, true_label = true_label)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}