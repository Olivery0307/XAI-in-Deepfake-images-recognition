{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"711124bc988c483182e547b57848b7dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9c22d583bb8412aa782e68c0c920c86","IPY_MODEL_628b1e6510cf46338d0e010fe095f2c4","IPY_MODEL_ff990731f4f645e2a8258dcecc41401a"],"layout":"IPY_MODEL_b20e68a1589942b4820b320bfd0a7b58"}},"a9c22d583bb8412aa782e68c0c920c86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_122561f392d14e3cafc6debb3187e24f","placeholder":"​","style":"IPY_MODEL_bfce42766f244ecd957c91c5ac56fd21","value":"preprocessor_config.json: 100%"}},"628b1e6510cf46338d0e010fe095f2c4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb2707c882e94a97b32088ae4d492133","max":160,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f572c2a50ae64510bbf92c40ace30fc2","value":160}},"ff990731f4f645e2a8258dcecc41401a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9dd59964a914d649291782d647463c3","placeholder":"​","style":"IPY_MODEL_ded7bcb289394a17a9ccb1f40e17fa0c","value":" 160/160 [00:00&lt;00:00, 13.2kB/s]"}},"b20e68a1589942b4820b320bfd0a7b58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"122561f392d14e3cafc6debb3187e24f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfce42766f244ecd957c91c5ac56fd21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb2707c882e94a97b32088ae4d492133":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f572c2a50ae64510bbf92c40ace30fc2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9dd59964a914d649291782d647463c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ded7bcb289394a17a9ccb1f40e17fa0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":13688968,"datasetId":8706215,"databundleVersionId":14431350}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set up","metadata":{"id":"BO5FnImoFEWY"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom transformers import ViTForImageClassification, TrainingArguments, Trainer, ViTImageProcessor, EarlyStoppingCallback, EvalPrediction\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom PIL import Image\nimport os\nimport io\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport sys\nimport random\nfrom collections import defaultdict\nimport glob","metadata":{"id":"mRdgDUABFyeF","executionInfo":{"status":"ok","timestamp":1762829608417,"user_tz":300,"elapsed":63210,"user":{"displayName":"LIANG-JIE CHIU","userId":"03072225676804136983"}},"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T04:32:53.538297Z","iopub.execute_input":"2025-11-12T04:32:53.538654Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"PATHS_TO_PREVIEW = [\n    (\"Celeb-real\", \"/kaggle/input/deepfake-images/data/Celeb-real\"),\n    (\"YouTube-real\", \"/kaggle/input/deepfake-images/data/YouTube-real\"),\n    (\"Celeb-synthesis\", \"/kaggle/input/deepfake-images/data/Celeb-synthesis\")\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preview_local_images(path_list):\n    \"\"\"\n    Fetches and displays one sample image from each local path.\n    path_list: list of tuples (label, path) or list of paths\n    \"\"\"\n    num_paths = len(path_list)\n    plt.figure(figsize=(15, 5))\n    \n    for i, item in enumerate(path_list):\n        # Handle both tuple (label, path) and plain path\n        if isinstance(item, tuple):\n            label, path = item\n        else:\n            path = item\n            label = os.path.basename(path)\n        \n        ax = plt.subplot(1, num_paths, i + 1)\n        patterns_to_check = [\".png\", \".jpg\", \".jpeg\"]\n        file_list = []\n        \n        for root, _, files in os.walk(path):\n            for file in files:\n                if any(file.lower().endswith(ext) for ext in patterns_to_check):\n                    file_list.append(os.path.join(root, file))\n        \n        if not file_list:\n            print(f\"No image files found in {gcs_path}\")\n            continue\n        \n        sample_file_path = file_list[0]\n        \n        try:\n            image = Image.open(sample_file_path).convert('RGB')\n            ax.imshow(image)\n            ax.set_title(f\"Class: {label}\\n{os.path.basename(sample_file_path)}\")\n            ax.axis('off')\n        except Exception as e:\n            print(f\"Error loading image {sample_file_path}: {e}\")\n            ax.set_title(f\"Error loading {label}\")\n            ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"id":"IZpaty3nG2fo","executionInfo":{"status":"ok","timestamp":1762829634513,"user_tz":300,"elapsed":35,"user":{"displayName":"LIANG-JIE CHIU","userId":"03072225676804136983"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preview_local_images(PATHS_TO_PREVIEW)","metadata":{"id":"szHnoGFzIWoa","outputId":"95d2417a-3a14-43a3-cbc9-a9193c01dd8c","executionInfo":{"status":"ok","timestamp":1762829654715,"user_tz":300,"elapsed":18409,"user":{"displayName":"LIANG-JIE CHIU","userId":"03072225676804136983"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{"id":"RbE3cpAmFnZ5"}},{"cell_type":"code","source":"class KaggleImageDataset(Dataset):\n    \"\"\"\n    Custom PyTorch Dataset to load images directly from the local file system\n    for use with HuggingFace Trainer.\n    \"\"\"\n    def __init__(self, file_paths, labels, processor):\n        self.file_paths = file_paths\n        self.labels = labels\n        self.processor = processor\n\n    def __len__(self):\n        \"\"\"Returns the total number of samples.\"\"\"\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Fetches the image from local path, applies processor, and returns\n        the sample in HuggingFace format.\n        \"\"\"\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        # Get the local path and label\n        local_path = self.file_paths[idx]\n        label = self.labels[idx]\n\n        try:\n            # Open the file from local path\n            image = Image.open(local_path).convert('RGB')\n\n            # Use the ViT processor\n            processed = self.processor(images=image, return_tensors=\"pt\")\n\n            # Extract the pixel values and remove the batch dimension\n            pixel_values = processed['pixel_values'].squeeze(0)\n\n            # Return in HuggingFace format\n            return {\n                'pixel_values': pixel_values,\n                'labels': torch.tensor(label, dtype=torch.long)\n            }\n\n        except Exception as e:\n            print(f\"Error loading image {local_path}: {e}\")\n            # Return a dummy sample if loading fails\n            dummy_image = Image.new('RGB', (224, 224), color='black')\n            processed = self.processor(images=dummy_image, return_tensors=\"pt\")\n            return {\n                'pixel_values': processed['pixel_values'].squeeze(0),\n                'labels': torch.tensor(0, dtype=torch.long)\n            }","metadata":{"id":"1aD4MTCrFqqu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_data_split_by_folder(real_paths, fake_paths, model_name, random_seed):\n    \"\"\"\n    Scans local directories, splits data BY FOLDER (video), creates datasets with ViT processor.\n    This ensures frames from the same video don't leak between train/val/test sets.\n    \"\"\"\n    \n    patterns_to_check = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n    \n    # Dictionary to store files grouped by their parent folder (video)\n    real_folders = defaultdict(list)\n    fake_folders = defaultdict(list)\n    \n    # Get REAL image paths grouped by folder\n    for path in real_paths:\n        for ext in patterns_to_check:\n            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n            for file in files:\n                # Get the parent folder name (video identifier)\n                parent_folder = os.path.dirname(file)\n                real_folders[parent_folder].append(file)\n        \n        print(f\"Found {len(real_folders)} REAL folders in {path}\")\n        total_real_files = sum(len(files) for files in real_folders.values())\n        print(f\"  Total REAL images: {total_real_files}\")\n    \n    # Get FAKE image paths grouped by folder\n    for path in fake_paths:\n        for ext in patterns_to_check:\n            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n            for file in files:\n                # Get the parent folder name (video identifier)\n                parent_folder = os.path.dirname(file)\n                fake_folders[parent_folder].append(file)\n        \n        print(f\"Found {len(fake_folders)} FAKE folders in {path}\")\n        total_fake_files = sum(len(files) for files in fake_folders.values())\n        print(f\"  Total FAKE images: {total_fake_files}\")\n    \n    if len(real_folders) == 0 and len(fake_folders) == 0:\n        raise ValueError(\"No images found in local paths. Check paths and permissions.\")\n    \n    # Split REAL folders into train/val/test (70/15/15)\n    real_folder_names = list(real_folders.keys())\n    train_real_folders, temp_real_folders = train_test_split(\n        real_folder_names, test_size=0.3, random_state=random_seed\n    )\n    val_real_folders, test_real_folders = train_test_split(\n        temp_real_folders, test_size=0.5, random_state=random_seed\n    )\n    \n    # Split FAKE folders into train/val/test (70/15/15)\n    fake_folder_names = list(fake_folders.keys())\n    train_fake_folders, temp_fake_folders = train_test_split(\n        fake_folder_names, test_size=0.3, random_state=random_seed\n    )\n    val_fake_folders, test_fake_folders = train_test_split(\n        temp_fake_folders, test_size=0.5, random_state=random_seed\n    )\n    \n    # Collect files and labels for each split\n    train_files, train_labels = [], []\n    val_files, val_labels = [], []\n    test_files, test_labels = [], []\n    \n    # Add REAL images to splits\n    for folder in train_real_folders:\n        train_files.extend(real_folders[folder])\n        train_labels.extend([LABEL_REAL] * len(real_folders[folder]))\n    \n    for folder in val_real_folders:\n        val_files.extend(real_folders[folder])\n        val_labels.extend([LABEL_REAL] * len(real_folders[folder]))\n    \n    for folder in test_real_folders:\n        test_files.extend(real_folders[folder])\n        test_labels.extend([LABEL_REAL] * len(real_folders[folder]))\n    \n    # Add FAKE images to splits\n    for folder in train_fake_folders:\n        train_files.extend(fake_folders[folder])\n        train_labels.extend([LABEL_FAKE] * len(fake_folders[folder]))\n    \n    for folder in val_fake_folders:\n        val_files.extend(fake_folders[folder])\n        val_labels.extend([LABEL_FAKE] * len(fake_folders[folder]))\n    \n    for folder in test_fake_folders:\n        test_files.extend(fake_folders[folder])\n        test_labels.extend([LABEL_FAKE] * len(fake_folders[folder]))\n    \n    # Print statistics\n    print(\"\\n\" + \"=\"*60)\n    print(\"SPLIT BY FOLDER (VIDEO) - STATISTICS\")\n    print(\"=\"*60)\n    print(f\"Real folders: {len(real_folder_names)} total\")\n    print(f\"  Train: {len(train_real_folders)} folders, {train_labels.count(LABEL_REAL)} images\")\n    print(f\"  Val:   {len(val_real_folders)} folders, {val_labels.count(LABEL_REAL)} images\")\n    print(f\"  Test:  {len(test_real_folders)} folders, {test_labels.count(LABEL_REAL)} images\")\n    print()\n    print(f\"Fake folders: {len(fake_folder_names)} total\")\n    print(f\"  Train: {len(train_fake_folders)} folders, {train_labels.count(LABEL_FAKE)} images\")\n    print(f\"  Val:   {len(val_fake_folders)} folders, {val_labels.count(LABEL_FAKE)} images\")\n    print(f\"  Test:  {len(test_fake_folders)} folders, {test_labels.count(LABEL_FAKE)} images\")\n    print()\n    print(f\"TOTAL:\")\n    print(f\"  Train: {len(train_files)} images ({len(train_real_folders) + len(train_fake_folders)} folders)\")\n    print(f\"  Val:   {len(val_files)} images ({len(val_real_folders) + len(val_fake_folders)} folders)\")\n    print(f\"  Test:  {len(test_files)} images ({len(test_real_folders) + len(test_fake_folders)} folders)\")\n    print(\"=\"*60)\n    \n    # Initialize ViT Image Processor\n    processor = ViTImageProcessor.from_pretrained(model_name)\n    \n    # Create Datasets with the processor\n    train_dataset = KaggleImageDataset(\n        file_paths=train_files,\n        labels=train_labels,\n        processor=processor\n    )\n    val_dataset = KaggleImageDataset(\n        file_paths=val_files,\n        labels=val_labels,\n        processor=processor\n    )\n    test_dataset = KaggleImageDataset(\n        file_paths=test_files,\n        labels=test_labels,\n        processor=processor\n    )\n    \n    return train_dataset, val_dataset, test_dataset, processor","metadata":{"id":"socv-cDOJv19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_logits_for_metrics(logits, labels):\n    pred_ids = torch.argmax(logits, dim=-1)\n    return pred_ids","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    \n    # Already converted to predictions, not logits\n    if isinstance(predictions, torch.Tensor):\n        predictions = predictions.cpu().numpy()\n    if isinstance(labels, torch.Tensor):\n        labels = labels.cpu().numpy()\n    \n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='binary'\n    )\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1\n    }","metadata":{"id":"TORZeyN6Qpx_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MemoryEfficientTrainer(Trainer):\n    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n        \"\"\"\n        Override to return predictions instead of full logits\n        \"\"\"\n        inputs = self._prepare_inputs(inputs)\n        \n        with torch.no_grad():\n            outputs = model(**inputs)\n            loss = outputs.loss\n            logits = outputs.logits\n        \n        # Return predictions instead of logits to save memory\n        if prediction_loss_only:\n            return (loss, None, None)\n        \n        # Convert to predictions immediately\n        preds = torch.argmax(logits, dim=-1)\n        labels = inputs.get(\"labels\")\n        \n        return (loss, preds, labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def attetion_rollout(attentions, discard_ratio=0.9):\n    \"\"\"\n    Compute attention rollout from all transformer layers.\n    Args:\n        attentions: tuple of attention tensors from each layer\n        discard_ratio: percentage of lowest attention values to discard\n    Returns:\n        Attention map for the [CLS] token\n    \"\"\"\n    # Get device from first attention tensor\n    device = attentions[0].device\n    \n    # Create identity matrix on the same device\n    result = torch.eye(attentions[0].size(-1)).to(device)\n    \n    for attention in attentions:\n        # Average across all heads\n        attention_heads_fused = attention.mean(dim=1)\n        attention_heads_fused = attention_heads_fused[0]\n        \n        # Drop the lowest attentions\n        flat = attention_heads_fused.view(-1)\n        _, indices = flat.topk(k=int(flat.size(-1) * discard_ratio), largest=False)\n        flat[indices] = 0\n        \n        # Normalize\n        I = torch.eye(attention_heads_fused.size(-1)).to(device)  # Fix: add .to(device)\n        a = (attention_heads_fused + 1.0 * I) / 2\n        a = a / a.sum(dim=-1, keepdim=True)\n        result = torch.matmul(a, result)\n    \n    mask = result[0, 1:]\n    return mask","metadata":{"id":"MZdo0m85x9UE","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_attention(model, image_path, processor, true_label=None):\n    \"\"\"\n    Visualize attention rollout for a single image.\n    \n    Args:\n        model: ViT model with output_attentions=True\n        image_path: Path to local image file\n        processor: ViTImageProcessor for preprocessing\n        true_label: Optional true label (0 for FAKE, 1 for REAL, or string)\n    \"\"\"\n    # Load image from local path\n    image = Image.open(image_path).convert('RGB')\n    \n    # Process image\n    inputs = processor(images=image, return_tensors=\"pt\")\n    \n    # Move inputs to same device as model\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # Get model outputs with attentions\n    model.eval()\n    with torch.no_grad():\n        outputs = model(**inputs, output_attentions=True)\n    \n    # Get attention weights\n    attentions = outputs.attentions  # tuple of (num_layers) tensors\n    \n    # Compute attention rollout\n    mask = attetion_rollout(attentions)\n    \n    # Reshape mask to image dimensions\n    num_patches = int(mask.shape[0] ** 0.5)\n    mask = mask.reshape(num_patches, num_patches).cpu().numpy()\n    \n    # Resize to original image size\n    mask = Image.fromarray((mask * 255).astype(np.uint8)).resize(\n        image.size, resample=Image.BILINEAR\n    )\n    mask = np.array(mask) / 255.0\n    \n    # Visualize\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    # Original image\n    axes[0].imshow(image)\n    axes[0].set_title('Original Image')\n    axes[0].axis('off')\n    \n    # Attention heatmap\n    axes[1].imshow(mask, cmap='jet')\n    axes[1].set_title('Attention Rollout')\n    axes[1].axis('off')\n    \n    # Overlay\n    axes[2].imshow(image)\n    axes[2].imshow(mask, cmap='jet', alpha=0.5)\n    axes[2].set_title('Overlay')\n    axes[2].axis('off')\n    \n    # Get prediction\n    prediction = outputs.logits.argmax(-1).item()\n    pred_label = model.config.id2label[prediction]\n    prob = torch.softmax(outputs.logits, dim=-1)[0][prediction].item()\n    \n    # Build title with prediction and true label\n    title_parts = [f'Prediction: {pred_label} ({prob:.2%})']\n    \n    if true_label is not None:\n        # Convert true_label to string if it's numeric\n        if isinstance(true_label, (int, np.integer)):\n            true_label_str = model.config.id2label[true_label]\n        else:\n            true_label_str = true_label\n        \n        # Check if prediction is correct\n        is_correct = (pred_label == true_label_str)\n        correctness = \"✓\" if is_correct else \"✗\"\n        \n        title_parts.append(f'True Label: {true_label_str} {correctness}')\n    \n    fig.suptitle(' | '.join(title_parts), fontsize=16)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return mask","metadata":{"id":"SW-RbGL81XRQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def seed_everything(seed: int):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"id":"LsCKdndrS27Q","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define Parameters and Input\nREAL_PATHS = [\n    \"/kaggle/input/deepfake-images/data/Celeb-real\",\n    \"/kaggle/input/deepfake-images/data/YouTube-real\"\n]\nFAKE_PATHS = [\n    \"/kaggle/input/deepfake-images/data/Celeb-synthesis\"\n]\n\nLABEL_REAL = 0\nLABEL_FAKE = 1\n\nIMG_SIZE = 224\nBATCH_SIZE = 16\nRANDOM_SEED = 42\nEPOCHS = 10\nLEARNING_RATE = 2e-5\n\nmodel_name = \"google/vit-base-patch16-224\"","metadata":{"id":"cuAo95zcRL5V","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed_everything(RANDOM_SEED)","metadata":{"id":"3SyLZKpcS1MK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the data\ntrain_dataset, val_dataset, test_dataset, processor = get_data_split_by_folder(\n    REAL_PATHS, FAKE_PATHS, model_name, random_seed = RANDOM_SEED\n)","metadata":{"id":"vBGXsxCVKdR9","outputId":"2a119f29-d678-46db-e551-584db4dc1f1d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"model = ViTForImageClassification.from_pretrained(\n    model_name,\n    num_labels = 2,\n    id2label = {0: \"FAKE\", 1: \"REAL\"},\n    label2id = {\"FAKE\": 0, \"REAL\": 1},\n    ignore_mismatched_sizes = True,\n    output_attentions = True\n)","metadata":{"id":"VMrtMHdRRXSJ","outputId":"c4c59d37-9f29-4901-8ff4-0e1a6815839e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for param in model.vit.embeddings.parameters():\n    param.requires_grad = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Freeze all encoder layers except the last 2\nnum_layers = len(model.vit.encoder.layer)\nfor i, layer in enumerate(model.vit.encoder.layer):\n    if i < num_layers - 2:  # Freeze all but last 2 layers\n        for param in layer.parameters():\n            param.requires_grad = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for param in model.vit.layernorm.parameters():\n    param.requires_grad = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping_callback = EarlyStoppingCallback(\n    early_stopping_patience = 3,\n    early_stopping_threshold = 0.01 \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir = \"./vit-fake-detector\",\n    per_device_train_batch_size = BATCH_SIZE,\n    per_device_eval_batch_size = 8,\n    num_train_epochs = EPOCHS,\n    eval_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate = LEARNING_RATE,\n    weight_decay = 0.01,\n    load_best_model_at_end = True,\n    metric_for_best_model = \"f1\",\n    greater_is_better = True,\n    logging_dir = './logs',\n    logging_steps = 100,\n    remove_unused_columns = False,\n    push_to_hub = False,\n    report_to = \"none\",\n    save_total_limit = 2,\n    dataloader_pin_memory = False\n)","metadata":{"id":"CSkCRB30QtLm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = MemoryEfficientTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset,\n    compute_metrics = compute_metrics,\n    callbacks = [early_stopping_callback]\n)","metadata":{"id":"39g9QFKGR2Aq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"qEsp09tJR75b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_results = trainer.evaluate(test_dataset)\nprint(f\"Test results: {test_results}\")","metadata":{"id":"9gj9rE-MR9Y8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processor = ViTImageProcessor.from_pretrained(model_name)","metadata":{"id":"h48Tt5pf1vKR","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model = trainer.model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n\nfor idx in tmp:\n    image_path = test_dataset.file_paths[idx]\n    true_label = test_dataset.labels[idx]\n    visualize_attention(best_model, image_path, processor, true_label = true_label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n\nfor idx in tmp:\n    image_path = test_dataset.file_paths[idx]\n    true_label = test_dataset.labels[idx]\n    visualize_attention(best_model, image_path, processor, true_label = true_label)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n\nfor idx in tmp:\n    image_path = test_dataset.file_paths[idx]\n    true_label = test_dataset.labels[idx]\n    visualize_attention(best_model, image_path, processor, true_label = true_label)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}