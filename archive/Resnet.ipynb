{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918f962c",
   "metadata": {
    "papermill": {
     "duration": 0.005581,
     "end_time": "2025-11-15T05:57:23.157607",
     "exception": false,
     "start_time": "2025-11-15T05:57:23.152026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fbaf2ea",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:23.167846Z",
     "iopub.status.busy": "2025-11-15T05:57:23.167558Z",
     "iopub.status.idle": "2025-11-15T05:57:32.875391Z",
     "shell.execute_reply": "2025-11-15T05:57:32.874736Z"
    },
    "papermill": {
     "duration": 9.714792,
     "end_time": "2025-11-15T05:57:32.876837",
     "exception": false,
     "start_time": "2025-11-15T05:57:23.162045",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from PIL import Image\n",
    "from torchvision import utils,transforms,models\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80759991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:32.887680Z",
     "iopub.status.busy": "2025-11-15T05:57:32.886715Z",
     "iopub.status.idle": "2025-11-15T05:57:32.894038Z",
     "shell.execute_reply": "2025-11-15T05:57:32.893332Z"
    },
    "papermill": {
     "duration": 0.013793,
     "end_time": "2025-11-15T05:57:32.895148",
     "exception": false,
     "start_time": "2025-11-15T05:57:32.881355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preview_local_images(path_list):\n",
    "    \"\"\"\n",
    "    Fetches and displays one sample image from each local path.\n",
    "    path_list: list of tuples (label, path) or list of paths\n",
    "    \"\"\"\n",
    "    num_paths = len(path_list)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, item in enumerate(path_list):\n",
    "        # Handle both tuple (label, path) and plain path\n",
    "        if isinstance(item, tuple):\n",
    "            label, path = item\n",
    "        else:\n",
    "            path = item\n",
    "            label = os.path.basename(path)\n",
    "        \n",
    "        ax = plt.subplot(1, num_paths, i + 1)\n",
    "        patterns_to_check = [\".png\", \".jpg\", \".jpeg\"]\n",
    "        file_list = []\n",
    "        \n",
    "        for root, _, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if any(file.lower().endswith(ext) for ext in patterns_to_check):\n",
    "                    file_list.append(os.path.join(root, file))\n",
    "        \n",
    "        if not file_list:\n",
    "            print(f\"No image files found in {file_list}\")\n",
    "            continue\n",
    "        \n",
    "        sample_file_path = file_list[0]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(sample_file_path).convert('RGB')\n",
    "            ax.imshow(image)\n",
    "            ax.set_title(f\"Class: {label}\\n{os.path.basename(sample_file_path)}\")\n",
    "            ax.axis('off')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {sample_file_path}: {e}\")\n",
    "            ax.set_title(f\"Error loading {label}\")\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c085dd",
   "metadata": {
    "papermill": {
     "duration": 0.004142,
     "end_time": "2025-11-15T05:57:32.903605",
     "exception": false,
     "start_time": "2025-11-15T05:57:32.899463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee040219",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:32.913271Z",
     "iopub.status.busy": "2025-11-15T05:57:32.912943Z",
     "iopub.status.idle": "2025-11-15T05:57:32.918868Z",
     "shell.execute_reply": "2025-11-15T05:57:32.918318Z"
    },
    "papermill": {
     "duration": 0.012267,
     "end_time": "2025-11-15T05:57:32.919962",
     "exception": false,
     "start_time": "2025-11-15T05:57:32.907695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to load images directly from the local file system\n",
    "    for use with a standard PyTorch training loop (e.g., with ResNet).\n",
    "    \"\"\"\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_paths (list): List of local file paths to images.\n",
    "            labels (list): List of corresponding labels (0 or 1).\n",
    "            transform (torchvision.transforms, optional): Transformations to apply to the image.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches the image from local path, applies transforms, and returns\n",
    "        a standard (image_tensor, label_tensor) tuple.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        local_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(local_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image_tensor = self.transform(image)\n",
    "            return image_tensor, torch.tensor(label, dtype=torch.long), local_path\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {local_path}: {e}\")\n",
    "            dummy_tensor = torch.zeros((3, 224, 224)) # Assuming 224x224\n",
    "            return dummy_tensor, torch.tensor(0, dtype=torch.long), \"ERROR_LOADING_FILE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01bd0566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:32.929579Z",
     "iopub.status.busy": "2025-11-15T05:57:32.929339Z",
     "iopub.status.idle": "2025-11-15T05:57:32.949683Z",
     "shell.execute_reply": "2025-11-15T05:57:32.949139Z"
    },
    "papermill": {
     "duration": 0.026644,
     "end_time": "2025-11-15T05:57:32.950758",
     "exception": false,
     "start_time": "2025-11-15T05:57:32.924114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_mixed_structure(\n",
    "    video_real_paths, video_fake_paths,\n",
    "    image_real_paths, image_fake_paths,\n",
    "    random_seed\n",
    "):\n",
    "    \"\"\"\n",
    "    Scans local directories and handles two types of datasets:\n",
    "    1. Video-based: Split BY FOLDER to prevent frame leakage\n",
    "    2. Image-based: Split BY IMAGE (no folder structure)\n",
    "    \n",
    "    Args:\n",
    "        video_real_paths: List of paths to real video folders (e.g., Celeb-real, YouTube-real)\n",
    "        video_fake_paths: List of paths to fake video folders (e.g., Celeb-synthesis)\n",
    "        image_real_paths: List of paths to real image folders (e.g., FFHQ-real-v2)\n",
    "        image_fake_paths: List of paths to fake image folders (e.g., StableDiffusion-fake-v2, stylegan-6000),\n",
    "        random_seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    patterns_to_check = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n",
    "    real_video_folders = defaultdict(list)\n",
    "    fake_video_folders = defaultdict(list)\n",
    "\n",
    "    # ========================================\n",
    "    # PART 1: Handle VIDEO-BASED datasets (split by folder)\n",
    "    # ========================================\n",
    "    for path in video_real_paths:\n",
    "        for ext in patterns_to_check:\n",
    "            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n",
    "            for file in files:\n",
    "                real_video_folders[os.path.dirname(file)].append(file)\n",
    "    \n",
    "    for path in video_fake_paths:\n",
    "        for ext in patterns_to_check:\n",
    "            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n",
    "            for file in files:\n",
    "                fake_video_folders[os.path.dirname(file)].append(file)\n",
    "\n",
    "    train_real_video_folders, val_real_video_folders, test_real_video_folders = [], [], []\n",
    "    train_fake_video_folders, val_fake_video_folders, test_fake_video_folders = [], [], []\n",
    "    \n",
    "    if len(real_video_folders) > 0:\n",
    "        real_folder_names = sorted(list(real_video_folders.keys()))\n",
    "        train_real_video_folders, temp_real = train_test_split(\n",
    "            real_folder_names, test_size=0.3, random_state=random_seed\n",
    "        )\n",
    "        val_real_video_folders, test_real_video_folders = train_test_split(\n",
    "            temp_real, test_size=0.5, random_state=random_seed\n",
    "        )\n",
    "    \n",
    "    if len(fake_video_folders) > 0:\n",
    "        fake_folder_names = sorted(list(fake_video_folders.keys()))\n",
    "        train_fake_video_folders, temp_fake = train_test_split(\n",
    "            fake_folder_names, test_size=0.3, random_state=random_seed\n",
    "        )\n",
    "        val_fake_video_folders, test_fake_video_folders = train_test_split(\n",
    "            temp_fake, test_size=0.5, random_state=random_seed\n",
    "        )\n",
    "\n",
    "    # ========================================\n",
    "    # PART 2: Handle IMAGE-BASED datasets (split by image)\n",
    "    # ========================================\n",
    "    real_image_files = []\n",
    "    fake_image_files = []\n",
    "    \n",
    "    for path in image_real_paths:\n",
    "        for ext in patterns_to_check:\n",
    "            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n",
    "            real_image_files.extend(files)\n",
    "    \n",
    "    for path in image_fake_paths:\n",
    "        for ext in patterns_to_check:\n",
    "            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n",
    "            fake_image_files.extend(files)\n",
    "    \n",
    "    train_real_images, val_real_images, test_real_images = [], [], []\n",
    "    train_fake_images, val_fake_images, test_fake_images = [], [], []\n",
    "    \n",
    "    if len(real_image_files) > 0:\n",
    "        sorted_real_image_files = sorted(real_image_files)\n",
    "        train_real_images, temp_real = train_test_split(\n",
    "            sorted_real_image_files, test_size=0.3, random_state=random_seed\n",
    "        )\n",
    "        val_real_images, test_real_images = train_test_split(\n",
    "            temp_real, test_size=0.5, random_state=random_seed\n",
    "        )\n",
    "    \n",
    "    if len(fake_image_files) > 0:\n",
    "        sorted_fake_image_files = sorted(fake_image_files)\n",
    "        train_fake_images, temp_fake = train_test_split(\n",
    "            sorted_fake_image_files, test_size=0.3, random_state=random_seed\n",
    "        )\n",
    "        val_fake_images, test_fake_images = train_test_split(\n",
    "            temp_fake, test_size=0.5, random_state=random_seed\n",
    "        )\n",
    "    \n",
    "    train_files, train_labels = [], []\n",
    "    val_files, val_labels = [], []\n",
    "    test_files, test_labels = [], []\n",
    "    \n",
    "    for folder in train_real_video_folders:\n",
    "        train_files.extend(real_video_folders[folder])\n",
    "        train_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n",
    "    for folder in val_real_video_folders:\n",
    "        val_files.extend(real_video_folders[folder])\n",
    "        val_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n",
    "    for folder in test_real_video_folders:\n",
    "        test_files.extend(real_video_folders[folder])\n",
    "        test_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n",
    "    \n",
    "    for folder in train_fake_video_folders:\n",
    "        train_files.extend(fake_video_folders[folder])\n",
    "        train_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n",
    "    for folder in val_fake_video_folders:\n",
    "        val_files.extend(fake_video_folders[folder])\n",
    "        val_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n",
    "    for folder in test_fake_video_folders:\n",
    "        test_files.extend(fake_video_folders[folder])\n",
    "        test_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n",
    "        \n",
    "    train_files.extend(train_real_images)\n",
    "    train_labels.extend([LABEL_REAL] * len(train_real_images))\n",
    "    val_files.extend(val_real_images)\n",
    "    val_labels.extend([LABEL_REAL] * len(val_real_images))\n",
    "    test_files.extend(test_real_images)\n",
    "    test_labels.extend([LABEL_REAL] * len(test_real_images))\n",
    "    \n",
    "    train_files.extend(train_fake_images)\n",
    "    train_labels.extend([LABEL_FAKE] * len(train_fake_images))\n",
    "    val_files.extend(val_fake_images)\n",
    "    val_labels.extend([LABEL_FAKE] * len(val_fake_images))\n",
    "    test_files.extend(test_fake_images)\n",
    "    test_labels.extend([LABEL_FAKE] * len(test_fake_images))\n",
    "\n",
    "    # ========================================\n",
    "    # PART 4: Print detailed statistics\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MIXED DATASET STATISTICS (Video-based + Image-based)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n--- VIDEO-BASED DATA (split by folder) ---\")\n",
    "    if len(real_video_folders) > 0:\n",
    "        print(f\"Real video folders: {len(real_video_folders)} total\")\n",
    "        print(f\"  Train: {len(train_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in train_real_video_folders)} frames\")\n",
    "        print(f\"  Val:   {len(val_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in val_real_video_folders)} frames\")\n",
    "        print(f\"  Test:  {len(test_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in test_real_video_folders)} frames\")\n",
    "    else:\n",
    "        print(\"No real video data\")\n",
    "    \n",
    "    if len(fake_video_folders) > 0:\n",
    "        print(f\"\\nFake video folders: {len(fake_video_folders)} total\")\n",
    "        print(f\"  Train: {len(train_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in train_fake_video_folders)} frames\")\n",
    "        print(f\"  Val:   {len(val_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in val_fake_video_folders)} frames\")\n",
    "        print(f\"  Test:  {len(test_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in test_fake_video_folders)} frames\")\n",
    "    else:\n",
    "        print(\"No fake video data\")\n",
    "    \n",
    "    print(\"\\n--- IMAGE-BASED DATA (split by image) ---\")\n",
    "    if len(real_image_files) > 0:\n",
    "        print(f\"Real images: {len(real_image_files)} total\")\n",
    "        print(f\"  Train: {len(train_real_images)} images\")\n",
    "        print(f\"  Val:   {len(val_real_images)} images\")\n",
    "        print(f\"  Test:  {len(test_real_images)} images\")\n",
    "    else:\n",
    "        print(\"No real image data\")\n",
    "    \n",
    "    if len(fake_image_files) > 0:\n",
    "        print(f\"\\nFake images: {len(fake_image_files)} total\")\n",
    "        print(f\"  Train: {len(train_fake_images)} images\")\n",
    "        print(f\"  Val:   {len(val_fake_images)} images\")\n",
    "        print(f\"  Test:  {len(test_fake_images)} images\")\n",
    "    else:\n",
    "        print(\"No fake image data\")\n",
    "    \n",
    "    print(\"\\n--- COMBINED TOTALS ---\")\n",
    "    print(f\"Train: {len(train_files)} total ({train_labels.count(LABEL_REAL)} real, {train_labels.count(LABEL_FAKE)} fake)\")\n",
    "    print(f\"Val:   {len(val_files)} total ({val_labels.count(LABEL_REAL)} real, {val_labels.count(LABEL_FAKE)} fake)\")\n",
    "    print(f\"Test:  {len(test_files)} total ({test_labels.count(LABEL_REAL)} real, {test_labels.count(LABEL_FAKE)} fake)\")\n",
    "    print(f\"\\nGrand Total: {len(train_files) + len(val_files) + len(test_files)} images\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ========================================\n",
    "    # PART 5: Create datasets\n",
    "    # ========================================\n",
    "    train_dataset = ImageDataset(\n",
    "        file_paths=train_files,\n",
    "        labels=train_labels,\n",
    "        transform=train_transform  # Pass the train_transform\n",
    "    )\n",
    "    val_dataset = ImageDataset(\n",
    "        file_paths=val_files,\n",
    "        labels=val_labels,\n",
    "        transform=val_test_transform   # Pass the val/test_transform\n",
    "    )\n",
    "    test_dataset = ImageDataset(\n",
    "        file_paths=test_files,\n",
    "        labels=test_labels,\n",
    "        transform=val_test_transform   # Pass the val/test_transform\n",
    "    )\n",
    "\n",
    "    # ========================================\n",
    "    # PART 6: Save test file paths for reproducibility\n",
    "    # ========================================\n",
    "    import json\n",
    "    \n",
    "    test_metadata = {\n",
    "        'random_seed': random_seed,\n",
    "        'test_files': test_files,\n",
    "        'test_labels': test_labels,\n",
    "        'num_test_samples': len(test_files),\n",
    "        'num_real': test_labels.count(LABEL_REAL),\n",
    "        'num_fake': test_labels.count(LABEL_FAKE),\n",
    "    }\n",
    "    \n",
    "    # Save to JSON file\n",
    "    metadata_path = f\"test_set_seed_{random_seed}.json\"\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(test_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ“ Test set metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0707962",
   "metadata": {
    "papermill": {
     "duration": 0.004232,
     "end_time": "2025-11-15T05:57:32.959277",
     "exception": false,
     "start_time": "2025-11-15T05:57:32.955045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f465860e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:32.968924Z",
     "iopub.status.busy": "2025-11-15T05:57:32.968436Z",
     "iopub.status.idle": "2025-11-15T05:57:32.972807Z",
     "shell.execute_reply": "2025-11-15T05:57:32.972328Z"
    },
    "papermill": {
     "duration": 0.010382,
     "end_time": "2025-11-15T05:57:32.973780",
     "exception": false,
     "start_time": "2025-11-15T05:57:32.963398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(model_name: str, pretrained: bool = True, device: str = 'cuda'):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained ResNet model (e.g., 'resnet34', 'resnet50') \n",
    "    from torchvision, adapts its final layer, and moves it to the device.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model: {model_name} (pretrained={pretrained})\")\n",
    "    \n",
    "    # Use 'DEFAULT' string for the newest weights\n",
    "    weights = 'DEFAULT' if pretrained else None\n",
    "    \n",
    "    try:\n",
    "        model = models.get_model(model_name, weights=weights)\n",
    "    except AttributeError:\n",
    "        print(f\"Error: Model '{model_name}' not found in torchvision.models.\")\n",
    "        raise\n",
    "\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, 2)\n",
    "    model = model.to(device)\n",
    "    print(f\"Model {model_name} is ready and on device: {device}\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "040b514f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:32.983222Z",
     "iopub.status.busy": "2025-11-15T05:57:32.982878Z",
     "iopub.status.idle": "2025-11-15T05:57:32.988272Z",
     "shell.execute_reply": "2025-11-15T05:57:32.987680Z"
    },
    "papermill": {
     "duration": 0.011358,
     "end_time": "2025-11-15T05:57:32.989362",
     "exception": false,
     "start_time": "2025-11-15T05:57:32.978004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Runs a single training epoch.\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_preds += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix(loss=(running_loss / total_samples), acc=(correct_preds / total_samples * 100.0))\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = (correct_preds / total_samples) * 100.0\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13b51885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:32.999279Z",
     "iopub.status.busy": "2025-11-15T05:57:32.998984Z",
     "iopub.status.idle": "2025-11-15T05:57:33.004298Z",
     "shell.execute_reply": "2025-11-15T05:57:33.003737Z"
    },
    "papermill": {
     "duration": 0.011814,
     "end_time": "2025-11-15T05:57:33.005411",
     "exception": false,
     "start_time": "2025-11-15T05:57:32.993597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_one_epoch(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Runs a single validation epoch.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            pbar.set_postfix(loss=(running_loss / total_samples), acc=(correct_preds / total_samples * 100.0))\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = (correct_preds / total_samples) * 100.0\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a812f390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:33.015255Z",
     "iopub.status.busy": "2025-11-15T05:57:33.014806Z",
     "iopub.status.idle": "2025-11-15T05:57:33.021147Z",
     "shell.execute_reply": "2025-11-15T05:57:33.020411Z"
    },
    "papermill": {
     "duration": 0.012556,
     "end_time": "2025-11-15T05:57:33.022355",
     "exception": false,
     "start_time": "2025-11-15T05:57:33.009799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main_training_loop(model, train_loader, val_loader, criterion, optimizer, num_epochs, model_save_path, device):\n",
    "    \"\"\"\n",
    "    Manages the main training and validation loop over N epochs,\n",
    "    saving the best performing model.\n",
    "    \"\"\"\n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Complete: \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Save the best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"New best model saved with Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f\"\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
    "    print(f\"Best Val Acc: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc52e3af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:33.032016Z",
     "iopub.status.busy": "2025-11-15T05:57:33.031759Z",
     "iopub.status.idle": "2025-11-15T05:57:33.040752Z",
     "shell.execute_reply": "2025-11-15T05:57:33.040042Z"
    },
    "papermill": {
     "duration": 0.015333,
     "end_time": "2025-11-15T05:57:33.041912",
     "exception": false,
     "start_time": "2025-11-15T05:57:33.026579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, test_dataset, device):\n",
    "    \"\"\"\n",
    "    Evaluates the final model on the test set and prints:\n",
    "    1. Overall classification report\n",
    "    2. Per-domain classification reports (Celeb-real, YouTube-real, FFHQ, etc.)\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting evaluation on test set...\")\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_file_paths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    # Get file paths from test_dataset\n",
    "    all_file_paths = test_dataset.file_paths\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. OVERALL REPORT\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"OVERALL TEST SET CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*70)\n",
    "    report = classification_report(\n",
    "        all_labels, all_preds, target_names=[\"REAL\", \"FAKE\"], digits=4\n",
    "    )\n",
    "    print(report)\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. PER-DOMAIN REPORTS\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PER-DOMAIN CLASSIFICATION REPORTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Define domain patterns (adjust these to match your actual paths)\n",
    "    domains = {\n",
    "        'Celeb-real': '/Celeb-real/',\n",
    "        'YouTube-real': '/YouTube-real/',\n",
    "        'Celeb-synthesis': '/Celeb-synthesis/',\n",
    "        'FFHQ-real': '/FFHQ-real',\n",
    "        'StableDiffusion-fake': '/StableDiffusion-fake',\n",
    "        'StyleGAN-fake': '/stylegan',\n",
    "    }\n",
    "    \n",
    "    for domain_name, domain_pattern in domains.items():\n",
    "        # Find indices that belong to this domain\n",
    "        domain_indices = [i for i, path in enumerate(all_file_paths) \n",
    "                         if domain_pattern in path]\n",
    "        \n",
    "        if len(domain_indices) == 0:\n",
    "            continue  # Skip if no samples from this domain\n",
    "        \n",
    "        # Extract predictions and labels for this domain\n",
    "        domain_labels = [all_labels[i] for i in domain_indices]\n",
    "        domain_preds = [all_preds[i] for i in domain_indices]\n",
    "        \n",
    "        # Print domain report\n",
    "        print(f\"\\n--- {domain_name} ({len(domain_indices)} samples) ---\")\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct = sum(1 for i in range(len(domain_labels)) \n",
    "                     if domain_labels[i] == domain_preds[i])\n",
    "        accuracy = correct / len(domain_labels) * 100\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.2f}% ({correct}/{len(domain_labels)})\")\n",
    "        \n",
    "        # Only print detailed report if both classes are present\n",
    "        unique_labels = set(domain_labels)\n",
    "        if len(unique_labels) > 1:\n",
    "            domain_report = classification_report(\n",
    "                domain_labels, domain_preds, \n",
    "                target_names=[\"REAL\", \"FAKE\"], \n",
    "                digits=4,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(domain_report)\n",
    "        else:\n",
    "            # Single class - just show confusion\n",
    "            label_name = \"REAL\" if 0 in unique_labels else \"FAKE\"\n",
    "            print(f\"  (All samples are {label_name})\")\n",
    "            print(f\"  True Positives: {correct}\")\n",
    "            print(f\"  False Negatives: {len(domain_labels) - correct}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac8133",
   "metadata": {
    "papermill": {
     "duration": 0.004072,
     "end_time": "2025-11-15T05:57:33.050293",
     "exception": false,
     "start_time": "2025-11-15T05:57:33.046221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c679502d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:33.059979Z",
     "iopub.status.busy": "2025-11-15T05:57:33.059371Z",
     "iopub.status.idle": "2025-11-15T05:57:33.063844Z",
     "shell.execute_reply": "2025-11-15T05:57:33.063117Z"
    },
    "papermill": {
     "duration": 0.010679,
     "end_time": "2025-11-15T05:57:33.065006",
     "exception": false,
     "start_time": "2025-11-15T05:57:33.054327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. Configuration ---\n",
    "REAL_CELEB_PATH = \"/kaggle/input/deepfake-images/Celeb-DF/data/Celeb-real\"\n",
    "REAL_YOUTUBE_PATH = \"/kaggle/input/deepfake-images/Celeb-DF/data/YouTube-real\"\n",
    "FAKE_CELEB_PATH = \"/kaggle/input/deepfake-images/Celeb-DF/data/Celeb-synthesis\"\n",
    "FFHQ_REAL_PATH = \"/kaggle/input/deepfake-images/FFHQ-real-v2/FFHQ-real-v2\"\n",
    "SD_PATH = \"/kaggle/input/deepfake-images/StableDiffusion-fake-v2/StableDiffusion-fake-v2\"\n",
    "GAN_PATH = \"/kaggle/input/stylegan-6000/kaggle/working/stylegan_fake_dataset_nvidia\"\n",
    "\n",
    "# Define labels\n",
    "LABEL_REAL = 0\n",
    "LABEL_FAKE = 1\n",
    "\n",
    "# Model and Training Hyperparameters\n",
    "MODEL_NAME = \"resnet34\"\n",
    "MODEL_SAVE_PATH = f\"/kaggle/working/best_{MODEL_NAME}_by_folder.pth\"\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 5 \n",
    "LEARNING_RATE = 1e-4\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769bb3da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:33.074785Z",
     "iopub.status.busy": "2025-11-15T05:57:33.074363Z",
     "iopub.status.idle": "2025-11-15T05:57:33.134614Z",
     "shell.execute_reply": "2025-11-15T05:57:33.133854Z"
    },
    "papermill": {
     "duration": 0.066545,
     "end_time": "2025-11-15T05:57:33.135834",
     "exception": false,
     "start_time": "2025-11-15T05:57:33.069289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Setup Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98a6c86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:33.146106Z",
     "iopub.status.busy": "2025-11-15T05:57:33.145601Z",
     "iopub.status.idle": "2025-11-15T05:57:33.149235Z",
     "shell.execute_reply": "2025-11-15T05:57:33.148675Z"
    },
    "papermill": {
     "duration": 0.009916,
     "end_time": "2025-11-15T05:57:33.150304",
     "exception": false,
     "start_time": "2025-11-15T05:57:33.140388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATHS_TO_PREVIEW = [\n",
    "    (\"Celeb-real\", REAL_CELEB_PATH),\n",
    "    (\"YouTube-real\", REAL_YOUTUBE_PATH),\n",
    "    (\"Celeb-synthesis\", FAKE_CELEB_PATH),\n",
    "    (\"FFHQ\", FFHQ_REAL_PATH),\n",
    "    (\"Stable-Diffusion\", SD_PATH),\n",
    "    (\"Style-GAN\", GAN_PATH )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943111c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:56:40.982593Z",
     "iopub.status.busy": "2025-11-15T05:56:40.982378Z",
     "iopub.status.idle": "2025-11-15T05:56:48.530632Z",
     "shell.execute_reply": "2025-11-15T05:56:48.529764Z",
     "shell.execute_reply.started": "2025-11-15T05:56:40.982570Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-11-15T05:57:33.154648",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preview_local_images(PATHS_TO_PREVIEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fbc1eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:56:48.531668Z",
     "iopub.status.busy": "2025-11-15T05:56:48.531414Z",
     "iopub.status.idle": "2025-11-15T05:56:48.536776Z",
     "shell.execute_reply": "2025-11-15T05:56:48.536245Z",
     "shell.execute_reply.started": "2025-11-15T05:56:48.531644Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 3. Define Image Transforms ---\n",
    "# Standard ImageNet normalization\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8676d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:56:48.537789Z",
     "iopub.status.busy": "2025-11-15T05:56:48.537480Z",
     "iopub.status.idle": "2025-11-15T05:57:02.936111Z",
     "shell.execute_reply": "2025-11-15T05:57:02.935345Z",
     "shell.execute_reply.started": "2025-11-15T05:56:48.537761Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 4. Create Datasets (Split by Folder) ---\n",
    "print(\"Loading and splitting data by folder...\")\n",
    "video_real_paths = [REAL_CELEB_PATH,REAL_YOUTUBE_PATH]\n",
    "video_fake_paths = [FAKE_CELEB_PATH]\n",
    "image_real_paths = [FFHQ_REAL_PATH]\n",
    "image_fake_paths = [SD_PATH,GAN_PATH]\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_data_mixed_structure(\n",
    "    video_real_paths = video_real_paths,\n",
    "    video_fake_paths = video_fake_paths,\n",
    "    image_real_paths = image_real_paths,\n",
    "    image_fake_paths = image_fake_paths,\n",
    "    random_seed = RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37656d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:02.937343Z",
     "iopub.status.busy": "2025-11-15T05:57:02.937019Z",
     "iopub.status.idle": "2025-11-15T05:57:02.944774Z",
     "shell.execute_reply": "2025-11-15T05:57:02.944049Z",
     "shell.execute_reply.started": "2025-11-15T05:57:02.937302Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 5. Create DataLoaders ---\n",
    "num_workers = 2 if device.type == 'cuda' else 0\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6084527b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:02.947110Z",
     "iopub.status.busy": "2025-11-15T05:57:02.946853Z",
     "iopub.status.idle": "2025-11-15T05:57:03.403892Z",
     "shell.execute_reply": "2025-11-15T05:57:03.403191Z",
     "shell.execute_reply.started": "2025-11-15T05:57:02.947094Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 6. Initialize Model, Loss, and Optimizer ---\n",
    "model = get_model(MODEL_NAME, pretrained=True, device=device)\n",
    "\n",
    "# Loss Function (CrossEntropyLoss is standard for classification)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Adam is a good default)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d848c95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T05:57:03.404730Z",
     "iopub.status.busy": "2025-11-15T05:57:03.404550Z",
     "iopub.status.idle": "2025-11-15T05:57:04.667159Z",
     "shell.execute_reply": "2025-11-15T05:57:04.666077Z",
     "shell.execute_reply.started": "2025-11-15T05:57:03.404716Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 7. Run Training ---\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "model, history = main_training_loop(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    model_save_path=MODEL_SAVE_PATH,\n",
    "    device=device\n",
    ")\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea49d2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T05:57:04.667860Z",
     "iopub.status.idle": "2025-11-15T05:57:04.668112Z",
     "shell.execute_reply": "2025-11-15T05:57:04.668011Z",
     "shell.execute_reply.started": "2025-11-15T05:57:04.668001Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 8. Run Final Testing ---\n",
    "print(f\"Loading best model from {MODEL_SAVE_PATH} for final testing...\")\n",
    "# Load the best model weights that were saved during training\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "\n",
    "\n",
    "domains = {\n",
    "    'Celeb-real (video)': '/kaggle/input/deepfake-images/Celeb-DF/data/Celeb-real',\n",
    "    'YouTube-real (video)': '/kaggle/input/deepfake-images/Celeb-DF/data/YouTube-real',\n",
    "    'Celeb-synthesis (video)': '/kaggle/input/deepfake-images/Celeb-DF/data/Celeb-synthesis',\n",
    "    'FFHQ-real (image)': '/kaggle/input/deepfake-images/FFHQ-real-v2/FFHQ-real-v2',\n",
    "    'StableDiffusion-fake (image)': '/kaggle/input/deepfake-images/StableDiffusion-fake-v2/StableDiffusion-fake-v2',\n",
    "    'StyleGAN-fake (image)': '/kaggle/input/stylegan-6000',\n",
    "}\n",
    "\n",
    "# Run evaluation on the test set\n",
    "test_model(model, test_loader, test_dataset, device)\n",
    "\n",
    "print(\"\\n--- Pipeline Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc91fcf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T05:57:04.669389Z",
     "iopub.status.idle": "2025-11-15T05:57:04.669631Z",
     "shell.execute_reply": "2025-11-15T05:57:04.669523Z",
     "shell.execute_reply.started": "2025-11-15T05:57:04.669511Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -l /kaggle/working/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f4680d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab7a2df",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T05:57:04.670686Z",
     "iopub.status.idle": "2025-11-15T05:57:04.670950Z",
     "shell.execute_reply": "2025-11-15T05:57:04.670824Z",
     "shell.execute_reply.started": "2025-11-15T05:57:04.670812Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install grad-cam\n",
    "import cv2\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af8dd17",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T05:57:04.671700Z",
     "iopub.status.idle": "2025-11-15T05:57:04.671923Z",
     "shell.execute_reply": "2025-11-15T05:57:04.671824Z",
     "shell.execute_reply.started": "2025-11-15T05:57:04.671815Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_gradcam(model, test_dataset, target_layer, device, num_images=5):\n",
    "    \"\"\"\n",
    "    Finds correct FAKE predictions, runs Grad-CAM++, and prints the\n",
    "    visualizations directly to the notebook.\n",
    "    \"\"\"\n",
    "    # Inverse normalization transform for displaying images\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "    \n",
    "    # Initialize Grad-CAM\n",
    "    cam =GradCAM(model=model, target_layers=[target_layer])\n",
    "    \n",
    "    # Target class is 1 (FAKE)\n",
    "    targets = [ClassifierOutputTarget(LABEL_FAKE)]\n",
    "    \n",
    "    # --- Find correctly predicted FAKE images ---\n",
    "    print(f\"Searching for {num_images} correctly predicted 'FAKE' images...\")\n",
    "    correct_fake_samples = []\n",
    "    \n",
    "    # Shuffle dataset to get random samples\n",
    "    indices = list(range(len(test_dataset)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(indices, desc=\"Finding samples\"):\n",
    "            \n",
    "            # --- THIS IS THE FIX ---\n",
    "            # Only unpack 2 items, not 3\n",
    "            image_tensor, label = test_dataset[idx]\n",
    "            # --- END FIX ---\n",
    "            \n",
    "            # Check if it's a FAKE image\n",
    "            if label == LABEL_FAKE:\n",
    "                # Get model prediction\n",
    "                output = model(image_tensor.unsqueeze(0).to(device))\n",
    "                pred = torch.argmax(output, dim=1).item()\n",
    "                \n",
    "                # Check if prediction was correct (pred == 1)\n",
    "                if pred == LABEL_FAKE:\n",
    "                    # Save the (unnormalized) tensor for visualization\n",
    "                    rgb_img = inv_normalize(image_tensor).permute(1, 2, 0).numpy()\n",
    "                    rgb_img = np.clip(rgb_img, 0, 1) # Clip values to be valid image\n",
    "                    \n",
    "                    correct_fake_samples.append((image_tensor, rgb_img))\n",
    "                    \n",
    "                    if len(correct_fake_samples) >= num_images:\n",
    "                        break\n",
    "                        \n",
    "    if not correct_fake_samples:\n",
    "        print(\"Could not find any correctly predicted 'FAKE' images to visualize.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(correct_fake_samples)} samples. Generating visualizations...\")\n",
    "\n",
    "    # --- Create the visualization plot ---\n",
    "    # (The rest of the function is identical and correct)\n",
    "    fig, axs = plt.subplots(len(correct_fake_samples), 3, figsize=(15, len(correct_fake_samples) * 5))\n",
    "    fig.suptitle(\"Grad-CAM++ Visualization for 'FAKE' Predictions\", fontsize=20, y=1.02)\n",
    "    \n",
    "    for i, (input_tensor, rgb_img) in enumerate(correct_fake_samples):\n",
    "        # Generate the CAM\n",
    "        input_tensor_cam = input_tensor.unsqueeze(0)\n",
    "        grayscale_cam = cam(input_tensor=input_tensor_cam, targets=targets)[0, :]\n",
    "        \n",
    "        # Create the overlay\n",
    "        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n",
    "        \n",
    "        # Plot Original\n",
    "        axs[i, 0].imshow(rgb_img)\n",
    "        axs[i, 0].set_title(f\"Sample {i+1}: Original Image\")\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        # Plot Heatmap\n",
    "        axs[i, 1].imshow(grayscale_cam, cmap='jet')\n",
    "        axs[i, 1].set_title(\"Grad-CAM++ Heatmap\")\n",
    "        axs[i, 1].axis('off')\n",
    "        \n",
    "        # Plot Overlay\n",
    "        axs[i, 2].imshow(visualization)\n",
    "        axs[i, 2].set_title(\"Overlay\")\n",
    "        axs[i, 2].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472ffb2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T05:57:04.672874Z",
     "iopub.status.idle": "2025-11-15T05:57:04.673169Z",
     "shell.execute_reply": "2025-11-15T05:57:04.673059Z",
     "shell.execute_reply.started": "2025-11-15T05:57:04.673046Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For ResNet-50, 'layer4' is the last convolutional block\n",
    "target_layer = model.layer4\n",
    "\n",
    "# --- Run Visualization ---\n",
    "visualize_gradcam(\n",
    "    model=model,\n",
    "    test_dataset=test_dataset,\n",
    "    target_layer=target_layer,\n",
    "    device=device,\n",
    "    num_images=10  # Display 10 examples\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24232b3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeec0e3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-15T05:57:04.674546Z",
     "iopub.status.idle": "2025-11-15T05:57:04.674856Z",
     "shell.execute_reply": "2025-11-15T05:57:04.674713Z",
     "shell.execute_reply.started": "2025-11-15T05:57:04.674697Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = \"/kaggle/input/resnet-34/pytorch/default/1\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8734376,
     "sourceId": 13728437,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8706215,
     "sourceId": 13736632,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 502495,
     "modelInstanceId": 487075,
     "sourceId": 645892,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-15T05:57:19.547704",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}