{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13728437,"sourceType":"datasetVersion","datasetId":8734376},{"sourceId":13736632,"sourceType":"datasetVersion","datasetId":8706215},{"sourceId":13822102,"sourceType":"datasetVersion","datasetId":8724954},{"sourceId":13822159,"sourceType":"datasetVersion","datasetId":8802348}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# Set up"],"metadata":{"id":"BO5FnImoFEWY"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from transformers import ViTForImageClassification, TrainingArguments, Trainer, ViTImageProcessor, EarlyStoppingCallback, AutoImageProcessor, SwinForImageClassification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, roc_auc_score\n","from PIL import Image, ImageFilter\n","import os\n","import io\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import sys\n","import random\n","from collections import defaultdict\n","import glob\n","from tqdm.notebook import tqdm"],"metadata":{"id":"mRdgDUABFyeF","executionInfo":{"status":"ok","timestamp":1762829608417,"user_tz":300,"elapsed":63210,"user":{"displayName":"LIANG-JIE CHIU","userId":"03072225676804136983"}},"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T22:59:23.119495Z","iopub.execute_input":"2025-11-21T22:59:23.119671Z","iopub.status.idle":"2025-11-21T22:59:58.537403Z","shell.execute_reply.started":"2025-11-21T22:59:23.119654Z","shell.execute_reply":"2025-11-21T22:59:58.536832Z"},"outputId":"66617aed-91c7-48d0-ef61-4a7dc76b5699"},"outputs":[{"name":"stderr","text":"2025-11-21 22:59:37.506693: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763765977.688317      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763765977.744641      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":null},{"cell_type":"markdown","source":["# Image Preprocessing"],"metadata":{"id":"RbE3cpAmFnZ5"}},{"cell_type":"code","source":["class KaggleImageDataset(Dataset):\n","    \"\"\"\n","    Custom PyTorch Dataset to load images directly from the local file system\n","    for use with HuggingFace Trainer, with optional augmentations.\n","    \"\"\"\n","    def __init__(self, file_paths, labels, processor, is_train=False):\n","        self.file_paths = file_paths\n","        self.labels = labels\n","        self.processor = processor\n","        self.is_train = is_train\n","\n","        # Define augmentations for training\n","        if self.is_train:\n","            self.augmentations = transforms.Compose([\n","                transforms.RandomHorizontalFlip(p=0.5),\n","                transforms.RandomApply([\n","                    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n","                ], p=0.3),\n","                transforms.RandomApply([\n","                    transforms.GaussianBlur(kernel_size=3)\n","                ], p=0.3),\n","                transforms.RandomRotation(degrees=10),\n","            ])\n","        else:\n","            self.augmentations = None\n","\n","    def __len__(self):\n","        \"\"\"Returns the total number of samples.\"\"\"\n","        return len(self.file_paths)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Fetches the image from local path, applies augmentations and processor,\n","        and returns the sample in HuggingFace format.\n","        \"\"\"\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        # Get the local path and label\n","        local_path = self.file_paths[idx]\n","        label = self.labels[idx]\n","\n","        try:\n","            # Open the file from local path\n","            image = Image.open(local_path).convert('RGB')\n","\n","            # Apply augmentations BEFORE the processor (only for training)\n","            if self.augmentations is not None:\n","                image = self.augmentations(image)\n","\n","            # Use the ViT processor (handles resizing and normalization)\n","            processed = self.processor(images=image, return_tensors=\"pt\")\n","\n","            # Extract the pixel values and remove the batch dimension\n","            pixel_values = processed['pixel_values'].squeeze(0)\n","\n","            # Return in HuggingFace format\n","            return {\n","                'pixel_values': pixel_values,\n","                'labels': torch.tensor(label, dtype=torch.long)\n","            }\n","\n","        except Exception as e:\n","            print(f\"Error loading image {local_path}: {e}\")\n","            # Return a dummy sample if loading fails\n","            dummy_image = Image.new('RGB', (224, 224), color='black')\n","            processed = self.processor(images=dummy_image, return_tensors=\"pt\")\n","            return {\n","                'pixel_values': processed['pixel_values'].squeeze(0),\n","                'labels': torch.tensor(0, dtype=torch.long)\n","            }"],"metadata":{"id":"1aD4MTCrFqqu","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T23:00:09.880191Z","iopub.execute_input":"2025-11-21T23:00:09.880499Z","iopub.status.idle":"2025-11-21T23:00:09.889518Z","shell.execute_reply.started":"2025-11-21T23:00:09.880478Z","shell.execute_reply":"2025-11-21T23:00:09.888653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def get_data_mixed_structure(video_real_paths, video_fake_paths,\n","                              image_real_paths, image_fake_paths,\n","                              model_name, random_seed):\n","    \"\"\"\n","    Scans local directories and handles two types of datasets:\n","    1. Video-based: Split BY FOLDER to prevent frame leakage\n","    2. Image-based: Split BY IMAGE (no folder structure)\n","\n","    Args:\n","        video_real_paths: List of paths to real video folders (e.g., Celeb-real, YouTube-real)\n","        video_fake_paths: List of paths to fake video folders (e.g., Celeb-synthesis)\n","        image_real_paths: List of paths to real image folders (e.g., FFHQ-real-v2)\n","        image_fake_paths: List of paths to fake image folders (e.g., StableDiffusion-fake-v2, stylegan-6000)\n","        model_name: HuggingFace model name for processor\n","        random_seed: Random seed for reproducibility\n","    \"\"\"\n","\n","    patterns_to_check = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n","\n","    # ========================================\n","    # PART 1: Handle VIDEO-BASED datasets (split by folder)\n","    # ========================================\n","    real_video_folders = defaultdict(list)\n","    fake_video_folders = defaultdict(list)\n","\n","    # Get REAL video folders\n","    for path in video_real_paths:\n","        for ext in patterns_to_check:\n","            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n","            for file in files:\n","                parent_folder = os.path.dirname(file)\n","                real_video_folders[parent_folder].append(file)\n","\n","        print(f\"Found {len(real_video_folders)} REAL video folders in {path}\")\n","        total_files = sum(len(files) for files in real_video_folders.values())\n","        print(f\"  Total REAL video frames: {total_files}\")\n","\n","    # Get FAKE video folders\n","    for path in video_fake_paths:\n","        for ext in patterns_to_check:\n","            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n","            for file in files:\n","                parent_folder = os.path.dirname(file)\n","                fake_video_folders[parent_folder].append(file)\n","\n","        print(f\"Found {len(fake_video_folders)} FAKE video folders in {path}\")\n","        total_files = sum(len(files) for files in fake_video_folders.values())\n","        print(f\"  Total FAKE video frames: {total_files}\")\n","\n","    # Split video folders (70/15/15)\n","    train_real_video_folders, val_real_video_folders, test_real_video_folders = [], [], []\n","    train_fake_video_folders, val_fake_video_folders, test_fake_video_folders = [], [], []\n","\n","    if len(real_video_folders) > 0:\n","        real_folder_names = list(real_video_folders.keys())\n","        train_real_video_folders, temp_real = train_test_split(\n","            real_folder_names, test_size=0.3, random_state=random_seed\n","        )\n","        val_real_video_folders, test_real_video_folders = train_test_split(\n","            temp_real, test_size=0.5, random_state=random_seed\n","        )\n","\n","    if len(fake_video_folders) > 0:\n","        fake_folder_names = list(fake_video_folders.keys())\n","        train_fake_video_folders, temp_fake = train_test_split(\n","            fake_folder_names, test_size=0.3, random_state=random_seed\n","        )\n","        val_fake_video_folders, test_fake_video_folders = train_test_split(\n","            temp_fake, test_size=0.5, random_state=random_seed\n","        )\n","\n","    # ========================================\n","    # PART 2: Handle IMAGE-BASED datasets (split by image)\n","    # ========================================\n","    real_image_files = []\n","    fake_image_files = []\n","\n","    # Get REAL image files\n","    for path in image_real_paths:\n","        for ext in patterns_to_check:\n","            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n","            real_image_files.extend(files)\n","        print(f\"Found {len([f for f in real_image_files if path in f])} REAL images in {path}\")\n","\n","    print(f\"  Total REAL images: {len(real_image_files)}\")\n","\n","    # Get FAKE image files\n","    for path in image_fake_paths:\n","        for ext in patterns_to_check:\n","            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n","            fake_image_files.extend(files)\n","        print(f\"Found {len([f for f in fake_image_files if path in f])} FAKE images in {path}\")\n","\n","    print(f\"  Total FAKE images: {len(fake_image_files)}\")\n","\n","    # Split image files (70/15/15)\n","    train_real_images, val_real_images, test_real_images = [], [], []\n","    train_fake_images, val_fake_images, test_fake_images = [], [], []\n","\n","    if len(real_image_files) > 0:\n","        train_real_images, temp_real = train_test_split(\n","            real_image_files, test_size=0.3, random_state=random_seed\n","        )\n","        val_real_images, test_real_images = train_test_split(\n","            temp_real, test_size=0.5, random_state=random_seed\n","        )\n","\n","    if len(fake_image_files) > 0:\n","        train_fake_images, temp_fake = train_test_split(\n","            fake_image_files, test_size=0.3, random_state=random_seed\n","        )\n","        val_fake_images, test_fake_images = train_test_split(\n","            temp_fake, test_size=0.5, random_state=random_seed\n","        )\n","\n","    # ========================================\n","    # PART 3: Combine video-based and image-based data\n","    # ========================================\n","    train_files, train_labels = [], []\n","    val_files, val_labels = [], []\n","    test_files, test_labels = [], []\n","\n","    # Add REAL VIDEO frames to splits\n","    for folder in train_real_video_folders:\n","        train_files.extend(real_video_folders[folder])\n","        train_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n","\n","    for folder in val_real_video_folders:\n","        val_files.extend(real_video_folders[folder])\n","        val_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n","\n","    for folder in test_real_video_folders:\n","        test_files.extend(real_video_folders[folder])\n","        test_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n","\n","    # Add FAKE VIDEO frames to splits\n","    for folder in train_fake_video_folders:\n","        train_files.extend(fake_video_folders[folder])\n","        train_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n","\n","    for folder in val_fake_video_folders:\n","        val_files.extend(fake_video_folders[folder])\n","        val_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n","\n","    for folder in test_fake_video_folders:\n","        test_files.extend(fake_video_folders[folder])\n","        test_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n","\n","    # Add REAL IMAGES to splits\n","    train_files.extend(train_real_images)\n","    train_labels.extend([LABEL_REAL] * len(train_real_images))\n","\n","    val_files.extend(val_real_images)\n","    val_labels.extend([LABEL_REAL] * len(val_real_images))\n","\n","    test_files.extend(test_real_images)\n","    test_labels.extend([LABEL_REAL] * len(test_real_images))\n","\n","    # Add FAKE IMAGES to splits\n","    train_files.extend(train_fake_images)\n","    train_labels.extend([LABEL_FAKE] * len(train_fake_images))\n","\n","    val_files.extend(val_fake_images)\n","    val_labels.extend([LABEL_FAKE] * len(val_fake_images))\n","\n","    test_files.extend(test_fake_images)\n","    test_labels.extend([LABEL_FAKE] * len(test_fake_images))\n","\n","    # ========================================\n","    # PART 4: Print detailed statistics\n","    # ========================================\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"MIXED DATASET STATISTICS (Video-based + Image-based)\")\n","    print(\"=\"*70)\n","\n","    print(\"\\n--- VIDEO-BASED DATA (split by folder) ---\")\n","    if len(real_video_folders) > 0:\n","        print(f\"Real video folders: {len(real_video_folders)} total\")\n","        print(f\"  Train: {len(train_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in train_real_video_folders)} frames\")\n","        print(f\"  Val:   {len(val_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in val_real_video_folders)} frames\")\n","        print(f\"  Test:  {len(test_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in test_real_video_folders)} frames\")\n","    else:\n","        print(\"No real video data\")\n","\n","    if len(fake_video_folders) > 0:\n","        print(f\"\\nFake video folders: {len(fake_video_folders)} total\")\n","        print(f\"  Train: {len(train_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in train_fake_video_folders)} frames\")\n","        print(f\"  Val:   {len(val_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in val_fake_video_folders)} frames\")\n","        print(f\"  Test:  {len(test_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in test_fake_video_folders)} frames\")\n","    else:\n","        print(\"No fake video data\")\n","\n","    print(\"\\n--- IMAGE-BASED DATA (split by image) ---\")\n","    if len(real_image_files) > 0:\n","        print(f\"Real images: {len(real_image_files)} total\")\n","        print(f\"  Train: {len(train_real_images)} images\")\n","        print(f\"  Val:   {len(val_real_images)} images\")\n","        print(f\"  Test:  {len(test_real_images)} images\")\n","    else:\n","        print(\"No real image data\")\n","\n","    if len(fake_image_files) > 0:\n","        print(f\"\\nFake images: {len(fake_image_files)} total\")\n","        print(f\"  Train: {len(train_fake_images)} images\")\n","        print(f\"  Val:   {len(val_fake_images)} images\")\n","        print(f\"  Test:  {len(test_fake_images)} images\")\n","    else:\n","        print(\"No fake image data\")\n","\n","    print(\"\\n--- COMBINED TOTALS ---\")\n","    print(f\"Train: {len(train_files)} total ({train_labels.count(LABEL_REAL)} real, {train_labels.count(LABEL_FAKE)} fake)\")\n","    print(f\"Val:   {len(val_files)} total ({val_labels.count(LABEL_REAL)} real, {val_labels.count(LABEL_FAKE)} fake)\")\n","    print(f\"Test:  {len(test_files)} total ({test_labels.count(LABEL_REAL)} real, {test_labels.count(LABEL_FAKE)} fake)\")\n","    print(f\"\\nGrand Total: {len(train_files) + len(val_files) + len(test_files)} images\")\n","    print(\"=\"*70)\n","\n","    # ========================================\n","    # PART 5: Create datasets\n","    # ========================================\n","    processor = AutoImageProcessor.from_pretrained(model_name, use_fast = True)\n","\n","    train_dataset = KaggleImageDataset(\n","        file_paths=train_files,\n","        labels=train_labels,\n","        processor=processor,\n","        is_train=True\n","    )\n","    val_dataset = KaggleImageDataset(\n","        file_paths=val_files,\n","        labels=val_labels,\n","        processor=processor,\n","        is_train=False\n","    )\n","    test_dataset = KaggleImageDataset(\n","        file_paths=test_files,\n","        labels=test_labels,\n","        processor=processor,\n","        is_train=False\n","    )\n","\n","    return train_dataset, val_dataset, test_dataset, processor"],"metadata":{"id":"socv-cDOJv19","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:36:11.948105Z","iopub.execute_input":"2025-11-16T00:36:11.948397Z","iopub.status.idle":"2025-11-16T00:36:11.970496Z","shell.execute_reply.started":"2025-11-16T00:36:11.948377Z","shell.execute_reply":"2025-11-16T00:36:11.969675Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","\n","    # Already converted to predictions, not logits\n","    if isinstance(predictions, torch.Tensor):\n","        predictions = predictions.cpu().numpy()\n","    if isinstance(labels, torch.Tensor):\n","        labels = labels.cpu().numpy()\n","\n","    accuracy = accuracy_score(labels, predictions)\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        labels, predictions, average='binary'\n","    )\n","\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1\n","    }"],"metadata":{"id":"TORZeyN6Qpx_","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:35:41.763235Z","iopub.execute_input":"2025-11-16T00:35:41.763554Z","iopub.status.idle":"2025-11-16T00:35:41.768722Z","shell.execute_reply.started":"2025-11-16T00:35:41.763512Z","shell.execute_reply":"2025-11-16T00:35:41.767844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["class MemoryEfficientTrainer(Trainer):\n","    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n","        \"\"\"\n","        Override to return predictions instead of full logits\n","        \"\"\"\n","        inputs = self._prepare_inputs(inputs)\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            loss = outputs.loss\n","            logits = outputs.logits\n","\n","        # Return predictions instead of logits to save memory\n","        if prediction_loss_only:\n","            return (loss, None, None)\n","\n","        # Convert to predictions immediately\n","        preds = torch.argmax(logits, dim=-1)\n","        labels = inputs.get(\"labels\")\n","\n","        return (loss, preds, labels)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:35:45.824423Z","iopub.execute_input":"2025-11-16T00:35:45.825071Z","iopub.status.idle":"2025-11-16T00:35:45.829978Z","shell.execute_reply.started":"2025-11-16T00:35:45.825042Z","shell.execute_reply":"2025-11-16T00:35:45.829124Z"},"id":"HKi-f35IP3YF"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def attetion_rollout(attentions, discard_ratio=0.9):\n","    \"\"\"\n","    Compute attention rollout from all transformer layers.\n","    Args:\n","        attentions: tuple of attention tensors from each layer\n","        discard_ratio: percentage of lowest attention values to discard\n","    Returns:\n","        Attention map for the [CLS] token\n","    \"\"\"\n","    # Get device from first attention tensor\n","    device = attentions[0].device\n","\n","    # Create identity matrix on the same device\n","    result = torch.eye(attentions[0].size(-1)).to(device)\n","\n","    for attention in attentions:\n","        # Average across all heads\n","        attention_heads_fused = attention.mean(dim=1)\n","        attention_heads_fused = attention_heads_fused[0]\n","\n","        # Drop the lowest attentions\n","        flat = attention_heads_fused.view(-1)\n","        _, indices = flat.topk(k=int(flat.size(-1) * discard_ratio), largest=False)\n","        flat[indices] = 0\n","\n","        # Normalize\n","        I = torch.eye(attention_heads_fused.size(-1)).to(device)  # Fix: add .to(device)\n","        a = (attention_heads_fused + 1.0 * I) / 2\n","        a = a / a.sum(dim=-1, keepdim=True)\n","        result = torch.matmul(a, result)\n","\n","    mask = result[0, 1:]\n","    return mask"],"metadata":{"id":"MZdo0m85x9UE","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:35:47.252352Z","iopub.execute_input":"2025-11-16T00:35:47.252965Z","iopub.status.idle":"2025-11-16T00:35:47.258450Z","shell.execute_reply.started":"2025-11-16T00:35:47.252943Z","shell.execute_reply":"2025-11-16T00:35:47.257669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def visualize_attention(model, image_path, processor, true_label=None):\n","    \"\"\"\n","    Visualize attention rollout for a single image.\n","\n","    Args:\n","        model: ViT model with output_attentions=True\n","        image_path: Path to local image file\n","        processor: ViTImageProcessor for preprocessing\n","        true_label: Optional true label (0 for FAKE, 1 for REAL, or string)\n","    \"\"\"\n","    # Load image from local path\n","    image = Image.open(image_path).convert('RGB')\n","\n","    # Process image\n","    inputs = processor(images=image, return_tensors=\"pt\")\n","\n","    # Move inputs to same device as model\n","    device = next(model.parameters()).device\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    # Get model outputs with attentions\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs, output_attentions=True)\n","\n","    # Get attention weights\n","    attentions = outputs.attentions  # tuple of (num_layers) tensors\n","\n","    # Compute attention rollout\n","    mask = attetion_rollout(attentions)\n","\n","    # Reshape mask to image dimensions\n","    num_patches = int(mask.shape[0] ** 0.5)\n","    mask = mask.reshape(num_patches, num_patches).cpu().numpy()\n","\n","    # Resize to original image size\n","    mask = Image.fromarray((mask * 255).astype(np.uint8)).resize(\n","        image.size, resample=Image.BILINEAR\n","    )\n","    mask = np.array(mask) / 255.0\n","\n","    # Visualize\n","    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n","\n","    # Original image\n","    axes[0].imshow(image)\n","    axes[0].set_title('Original Image')\n","    axes[0].axis('off')\n","\n","    # Attention heatmap\n","    axes[1].imshow(mask, cmap='jet')\n","    axes[1].set_title('Attention Rollout')\n","    axes[1].axis('off')\n","\n","    # Overlay\n","    axes[2].imshow(image)\n","    axes[2].imshow(mask, cmap='jet', alpha=0.5)\n","    axes[2].set_title('Overlay')\n","    axes[2].axis('off')\n","\n","    # Get prediction\n","    prediction = outputs.logits.argmax(-1).item()\n","    pred_label = model.config.id2label[prediction]\n","    prob = torch.softmax(outputs.logits, dim=-1)[0][prediction].item()\n","\n","    # Build title with prediction and true label\n","    title_parts = [f'Prediction: {pred_label} ({prob:.2%})']\n","\n","    if true_label is not None:\n","        # Convert true_label to string if it's numeric\n","        if isinstance(true_label, (int, np.integer)):\n","            true_label_str = model.config.id2label[true_label]\n","        else:\n","            true_label_str = true_label\n","\n","        # Check if prediction is correct\n","        is_correct = (pred_label == true_label_str)a\n","        correctness = \"✓\" if is_correct else \"✗\"\n","\n","        title_parts.append(f'True Label: {true_label_str} {correctness}')\n","\n","    fig.suptitle(' | '.join(title_parts), fontsize=16)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    return mask"],"metadata":{"id":"SW-RbGL81XRQ","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:35:49.149682Z","iopub.execute_input":"2025-11-16T00:35:49.149970Z","iopub.status.idle":"2025-11-16T00:35:49.159307Z","shell.execute_reply.started":"2025-11-16T00:35:49.149948Z","shell.execute_reply":"2025-11-16T00:35:49.158511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def visualize_attention_per_domain(model, test_dataset, processor, n_samples=10):\n","    \"\"\"\n","    Visualize attention rollout for random samples from each domain.\n","\n","    Args:\n","        model: ViT model with output_attentions=True\n","        test_dataset: Test dataset\n","        processor: ViTImageProcessor\n","        n_samples: Number of samples to visualize per domain\n","        random_seed: Random seed for reproducibility\n","    \"\"\"\n","    # Define domains\n","    domains = {\n","        'Celeb-real (video)': 'Celeb-real',\n","        'YouTube-real (video)': 'YouTube-real',\n","        'Celeb-synthesis (video)': 'Celeb-synthesis',\n","        'FFHQ-real (image)': 'FFHQ-real-v2',\n","        'StableDiffusion-fake (image)': 'StableDiffusion-fake-v2',\n","        'StyleGAN-fake (image)': 'stylegan',\n","    }\n","\n","    paths = test_dataset.file_paths\n","    labels = test_dataset.labels\n","\n","    # Process each domain\n","    for domain_name, pattern in domains.items():\n","        print(\"\\n\" + \"=\"*70)\n","        print(f\"VISUALIZING: {domain_name}\")\n","        print(\"=\"*70)\n","\n","        # Find indices for this domain\n","        domain_indices = [i for i, p in enumerate(paths) if pattern in p]\n","\n","        if len(domain_indices) == 0:\n","            print(f\"No samples found for {domain_name}\")\n","            continue\n","\n","        print(f\"Total samples in domain: {len(domain_indices)}\")\n","\n","        # Sample random indices\n","        n_to_sample = min(n_samples, len(domain_indices))\n","        sampled_indices = np.random.choice(domain_indices, size=n_to_sample, replace=False)\n","\n","        print(f\"Visualizing {n_to_sample} random samples...\\n\")\n","\n","        # Visualize each sample\n","        for idx in sampled_indices:\n","            image_path = paths[idx]\n","            true_label = labels[idx]\n","\n","            print(f\"Sample {idx}: {image_path.split('/')[-1]}\")\n","            visualize_attention(model, image_path, processor, true_label=true_label)\n","            print()  # Add spacing between visualizations"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:52:57.860738Z","iopub.execute_input":"2025-11-16T00:52:57.861464Z","iopub.status.idle":"2025-11-16T00:52:57.868202Z","shell.execute_reply.started":"2025-11-16T00:52:57.861439Z","shell.execute_reply":"2025-11-16T00:52:57.867641Z"},"id":"h9YEAIU4P3YF"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def evaluate_per_domain(trainer, test_dataset):\n","    \"\"\"\n","    Simpler version using trainer.predict()\n","    \"\"\"\n","    from sklearn.metrics import classification_report\n","\n","    # Get predictions\n","    predictions = trainer.predict(test_dataset)\n","    preds = predictions.predictions\n","    labels = predictions.label_ids     # Shape: (n_samples,)\n","    paths = test_dataset.file_paths\n","\n","    # Overall report\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"OVERALL RESULTS\")\n","    print(\"=\"*70)\n","    print(classification_report(labels, preds, target_names=[\"FAKE\", \"REAL\"], digits=4))\n","\n","    # Per-domain analysis\n","    domains = {\n","        'Celeb-real': 'Celeb-real',\n","        'YouTube-real': 'YouTube-real',\n","        'Celeb-synthesis': 'Celeb-synthesis',\n","        'FFHQ-real': 'FFHQ-real',\n","        'StableDiffusion-fake': 'StableDiffusion-fake-v2',\n","        'StyleGAN-fake': 'stylegan',\n","    }\n","\n","    for domain_name, pattern in domains.items():\n","        indices = [i for i, p in enumerate(paths) if pattern in p]\n","        if not indices:\n","            continue\n","\n","        domain_labels = [labels[i] for i in indices]\n","        domain_preds = [preds[i] for i in indices]\n","\n","        print(f\"\\n--- {domain_name} ({len(indices)} samples) ---\")\n","        if len(set(domain_labels)) > 1:\n","            print(classification_report(domain_labels, domain_preds,\n","                                       target_names=[\"FAKE\", \"REAL\"], digits=4))\n","        else:\n","            acc = sum(1 for i in range(len(domain_labels)) if domain_labels[i] == domain_preds[i])\n","            print(f\"Accuracy: {acc/len(domain_labels)*100:.2f}%\")\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:04:15.028991Z","iopub.execute_input":"2025-11-16T01:04:15.029597Z","iopub.status.idle":"2025-11-16T01:04:15.036696Z","shell.execute_reply.started":"2025-11-16T01:04:15.029572Z","shell.execute_reply":"2025-11-16T01:04:15.035965Z"},"id":"dAHTZSX1P3YF"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["def seed_everything(seed: int):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"],"metadata":{"id":"LsCKdndrS27Q","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:35:57.508213Z","iopub.execute_input":"2025-11-16T00:35:57.508957Z","iopub.status.idle":"2025-11-16T00:35:57.513264Z","shell.execute_reply.started":"2025-11-16T00:35:57.508932Z","shell.execute_reply":"2025-11-16T00:35:57.512462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# Define Parameters and Input\n","video_real_paths = [\n","    \"/kaggle/input/deepfake-images/Celeb-DF/data/Celeb-real\",\n","    \"/kaggle/input/deepfake-images/Celeb-DF/data/YouTube-real\"\n","]\n","video_fake_paths = [\n","    \"/kaggle/input/deepfake-images/Celeb-DF/data/Celeb-synthesis\"\n","]\n","\n","image_real_paths = [\n","    \"/kaggle/input/deepfake-images/FFHQ-real-v2/FFHQ-real-v2\"\n","]\n","\n","image_fake_paths = [\n","    \"/kaggle/input/deepfake-images/StableDiffusion-fake-v2/StableDiffusion-fake-v2\",\n","    \"/kaggle/input/stylegan-6000/kaggle/working/stylegan_fake_dataset_nvidia\"\n","]\n","\n","LABEL_REAL = 0\n","LABEL_FAKE = 1\n","\n","IMG_SIZE = 224\n","BATCH_SIZE = 16\n","RANDOM_SEED = 42\n","EPOCHS = 5\n","LEARNING_RATE = 2e-5\n","\n","model_name = \"google/vit-base-patch16-224\""],"metadata":{"id":"cuAo95zcRL5V","trusted":true,"execution":{"iopub.status.busy":"2025-11-21T23:00:42.074273Z","iopub.execute_input":"2025-11-21T23:00:42.074638Z","iopub.status.idle":"2025-11-21T23:00:42.079975Z","shell.execute_reply.started":"2025-11-21T23:00:42.074607Z","shell.execute_reply":"2025-11-21T23:00:42.079090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["seed_everything(RANDOM_SEED)"],"metadata":{"id":"3SyLZKpcS1MK","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:36:01.549180Z","iopub.execute_input":"2025-11-16T00:36:01.549482Z","iopub.status.idle":"2025-11-16T00:36:01.557257Z","shell.execute_reply.started":"2025-11-16T00:36:01.549460Z","shell.execute_reply":"2025-11-16T00:36:01.556715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["train_dataset, val_dataset, test_dataset, processor = get_data_mixed_structure(\n","    video_real_paths = video_real_paths,\n","    video_fake_paths = video_fake_paths,\n","    image_real_paths = image_real_paths,\n","    image_fake_paths = image_fake_paths,\n","    model_name = model_name,\n","    random_seed = RANDOM_SEED\n",")"],"metadata":{"id":"vBGXsxCVKdR9","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:36:21.623603Z","iopub.execute_input":"2025-11-16T00:36:21.624346Z","iopub.status.idle":"2025-11-16T00:37:47.466424Z","shell.execute_reply.started":"2025-11-16T00:36:21.624316Z","shell.execute_reply":"2025-11-16T00:37:47.465564Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["print(f\"Training dataset size: {len(train_dataset)}\")\n","print(f\"Validation dataset size: {len(val_dataset)}\")\n","print(f\"Test dataset size: {len(test_dataset)}\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:37:47.467711Z","iopub.execute_input":"2025-11-16T00:37:47.467985Z","iopub.status.idle":"2025-11-16T00:37:47.473250Z","shell.execute_reply.started":"2025-11-16T00:37:47.467961Z","shell.execute_reply":"2025-11-16T00:37:47.472214Z"},"id":"WLGFmqUSP3YG"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Modeling"],"metadata":{"id":"44CQOsGkP3YG"}},{"cell_type":"code","source":["model = ViTForImageClassification.from_pretrained(\n","    model_name,\n","    num_labels = 2,\n","    id2label = {0: \"REAL\", 1: \"FAKE\"},\n","    label2id = {\"REAL\": 0, \"FAKE\": 1},\n","    ignore_mismatched_sizes = True,\n","    output_attentions = True\n",")"],"metadata":{"id":"VMrtMHdRRXSJ","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:20:50.791324Z","iopub.execute_input":"2025-11-15T17:20:50.791581Z","iopub.status.idle":"2025-11-15T17:20:53.513636Z","shell.execute_reply.started":"2025-11-15T17:20:50.791560Z","shell.execute_reply":"2025-11-15T17:20:53.513034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["for param in model.vit.embeddings.parameters():\n","    param.requires_grad = False\n","\n","# Freeze all encoder layers except the last 2\n","num_layers = len(model.vit.encoder.layer)\n","for i, layer in enumerate(model.vit.encoder.layer):\n","    if i < num_layers - 2:  # Freeze all but last 2 layers\n","        for param in layer.parameters():\n","            param.requires_grad = False\n","\n","for param in model.vit.layernorm.parameters():\n","    param.requires_grad = True"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:20:53.515302Z","iopub.execute_input":"2025-11-15T17:20:53.515518Z","iopub.status.idle":"2025-11-15T17:20:53.520629Z","shell.execute_reply.started":"2025-11-15T17:20:53.515502Z","shell.execute_reply":"2025-11-15T17:20:53.519890Z"},"id":"YepU4EBBP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:20:53.521447Z","iopub.execute_input":"2025-11-15T17:20:53.521780Z","iopub.status.idle":"2025-11-15T17:20:53.539825Z","shell.execute_reply.started":"2025-11-15T17:20:53.521747Z","shell.execute_reply":"2025-11-15T17:20:53.539028Z"},"id":"sjOJsutmP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["early_stopping_callback = EarlyStoppingCallback(\n","    early_stopping_patience = 3,\n","    early_stopping_threshold = 0.001\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:20:53.540661Z","iopub.execute_input":"2025-11-15T17:20:53.541210Z","iopub.status.idle":"2025-11-15T17:20:53.556995Z","shell.execute_reply.started":"2025-11-15T17:20:53.541190Z","shell.execute_reply":"2025-11-15T17:20:53.556426Z"},"id":"4vPyVG5BP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir = \"./vit-fake-detector_freezed\",\n","    per_device_train_batch_size = BATCH_SIZE,\n","    per_device_eval_batch_size = 8,\n","    num_train_epochs = EPOCHS,\n","    eval_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate = LEARNING_RATE,\n","    weight_decay = 0.01,\n","    warmup_ratio = 0.1,\n","    load_best_model_at_end = True,\n","    metric_for_best_model = \"f1\",\n","    greater_is_better = True,\n","    logging_dir = './logs',\n","    logging_steps = 100,\n","    remove_unused_columns = False,\n","    push_to_hub = False,\n","    report_to = \"none\",\n","    save_total_limit = 2,\n","    dataloader_pin_memory = False\n",")"],"metadata":{"id":"CSkCRB30QtLm","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:20:53.557706Z","iopub.execute_input":"2025-11-15T17:20:53.557914Z","iopub.status.idle":"2025-11-15T17:20:53.595419Z","shell.execute_reply.started":"2025-11-15T17:20:53.557898Z","shell.execute_reply":"2025-11-15T17:20:53.594651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["trainer = MemoryEfficientTrainer(\n","    model = model,\n","    args = training_args,\n","    train_dataset = train_dataset,\n","    eval_dataset = val_dataset,\n","    compute_metrics = compute_metrics,\n","    callbacks = [early_stopping_callback]\n",")"],"metadata":{"id":"39g9QFKGR2Aq","trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:20:53.596233Z","iopub.execute_input":"2025-11-15T17:20:53.596688Z","iopub.status.idle":"2025-11-15T17:20:53.890034Z","shell.execute_reply.started":"2025-11-15T17:20:53.596659Z","shell.execute_reply":"2025-11-15T17:20:53.889419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["import gc\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T17:20:53.890836Z","iopub.execute_input":"2025-11-15T17:20:53.891065Z","iopub.status.idle":"2025-11-15T17:20:54.259124Z","shell.execute_reply.started":"2025-11-15T17:20:53.891047Z","shell.execute_reply":"2025-11-15T17:20:54.258470Z"},"id":"zIFbpHYuP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"qEsp09tJR75b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":["test_results = trainer.evaluate(test_dataset)\n","print(f\"Test results: {test_results}\")"],"metadata":{"id":"9gj9rE-MR9Y8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":["model = trainer.model"],"metadata":{"trusted":true,"id":"OTDz4rdsP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["for param in model.parameters():\n","    param.requires_grad = True"],"metadata":{"trusted":true,"id":"U2hwxi-RP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["early_stopping_callback = EarlyStoppingCallback(\n","    early_stopping_patience = 3,\n","    early_stopping_threshold = 0.001\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:38:40.136625Z","iopub.execute_input":"2025-11-16T00:38:40.137249Z","iopub.status.idle":"2025-11-16T00:38:40.140671Z","shell.execute_reply.started":"2025-11-16T00:38:40.137226Z","shell.execute_reply":"2025-11-16T00:38:40.139828Z"},"id":"prCzgcqbP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir = \"./vit-fake-detector_unfreezed\",\n","    per_device_train_batch_size = BATCH_SIZE,\n","    per_device_eval_batch_size = 8,\n","    num_train_epochs = EPOCHS,\n","    eval_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate = LEARNING_RATE,\n","    weight_decay = 0.01,\n","    warmup_ratio = 0.1,\n","    load_best_model_at_end = True,\n","    metric_for_best_model = \"f1\",\n","    greater_is_better = True,\n","    logging_dir = './logs',\n","    logging_steps = 100,\n","    remove_unused_columns = False,\n","    push_to_hub = False,\n","    report_to = \"none\",\n","    save_total_limit = 2,\n","    dataloader_pin_memory = False\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:38:31.003081Z","iopub.execute_input":"2025-11-16T00:38:31.003375Z","iopub.status.idle":"2025-11-16T00:38:31.034246Z","shell.execute_reply.started":"2025-11-16T00:38:31.003353Z","shell.execute_reply":"2025-11-16T00:38:31.033474Z"},"id":"SEcod2dQP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["trainer = MemoryEfficientTrainer(\n","    model = model,\n","    args = training_args,\n","    train_dataset = train_dataset,\n","    eval_dataset = val_dataset,\n","    compute_metrics = compute_metrics,\n","    callbacks = [early_stopping_callback]\n",")"],"metadata":{"trusted":true,"id":"9gCOzeV-P3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["import gc\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"trusted":true,"id":"clQS8pCTP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["trainer.train()"],"metadata":{"trusted":true,"id":"vWZvkt1dP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["test_results = trainer.evaluate(test_dataset)\n","print(f\"Test results: {test_results}\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:38:48.718659Z","iopub.execute_input":"2025-11-16T00:38:48.719302Z","iopub.status.idle":"2025-11-16T00:42:52.944072Z","shell.execute_reply.started":"2025-11-16T00:38:48.719273Z","shell.execute_reply":"2025-11-16T00:42:52.943417Z"},"id":"cONAF-A8P3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["evaluate_per_domain(trainer, test_dataset)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T01:04:22.031471Z","iopub.execute_input":"2025-11-16T01:04:22.032049Z","iopub.status.idle":"2025-11-16T01:07:14.479926Z","shell.execute_reply.started":"2025-11-16T01:04:22.032027Z","shell.execute_reply":"2025-11-16T01:07:14.479060Z"},"id":"DFtIocV3P3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["best_model = trainer.model"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:42:52.945049Z","iopub.execute_input":"2025-11-16T00:42:52.945319Z","iopub.status.idle":"2025-11-16T00:42:52.948964Z","shell.execute_reply.started":"2025-11-16T00:42:52.945302Z","shell.execute_reply":"2025-11-16T00:42:52.948189Z"},"id":"Bxs1adUQP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n","\n","for idx in tmp:\n","    image_path = test_dataset.file_paths[idx]\n","    true_label = test_dataset.labels[idx]\n","    visualize_attention(best_model, image_path, processor, true_label = true_label)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:45:44.176722Z","iopub.execute_input":"2025-11-16T00:45:44.176931Z","iopub.status.idle":"2025-11-16T00:45:49.083418Z","shell.execute_reply.started":"2025-11-16T00:45:44.176914Z","shell.execute_reply":"2025-11-16T00:45:49.082324Z"},"id":"sS-IURgMP3YG"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n","\n","for idx in tmp:\n","    image_path = test_dataset.file_paths[idx]\n","    true_label = test_dataset.labels[idx]\n","    visualize_attention(best_model, image_path, processor, true_label = true_label)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:45:49.084479Z","iopub.execute_input":"2025-11-16T00:45:49.084838Z","iopub.status.idle":"2025-11-16T00:45:55.465114Z","shell.execute_reply.started":"2025-11-16T00:45:49.084817Z","shell.execute_reply":"2025-11-16T00:45:55.464407Z"},"id":"IPYb00j_P3YH"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n","\n","for idx in tmp:\n","    image_path = test_dataset.file_paths[idx]\n","    true_label = test_dataset.labels[idx]\n","    visualize_attention(best_model, image_path, processor, true_label = true_label)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:45:55.466133Z","iopub.execute_input":"2025-11-16T00:45:55.466602Z","iopub.status.idle":"2025-11-16T00:46:00.349625Z","shell.execute_reply.started":"2025-11-16T00:45:55.466575Z","shell.execute_reply":"2025-11-16T00:46:00.348937Z"},"id":"515GZSG9P3YH"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["visualize_attention_per_domain(\n","    model=best_model,\n","    test_dataset=test_dataset,\n","    processor=processor,\n","    n_samples=10,\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T00:53:11.097315Z","iopub.execute_input":"2025-11-16T00:53:11.097610Z","iopub.status.idle":"2025-11-16T00:53:47.311692Z","shell.execute_reply.started":"2025-11-16T00:53:11.097590Z","shell.execute_reply":"2025-11-16T00:53:47.310806Z"},"id":"go_3eudbP3YJ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["## Hold-out Testing"],"metadata":{"id":"-DEYVPtiP3YK"}},{"cell_type":"code","source":["def evaluate_holdout_set(model, holdout_real_paths, holdout_fake_paths,\n","                         processor, batch_size=32, model_name=\"ViT\"):\n","    \"\"\"\n","    Evaluate model on a new hold-out testing set (pure images, not from videos).\n","\n","    Args:\n","        model: Trained ViT model\n","        holdout_real_paths: List of paths to real image folders\n","        holdout_fake_paths: List of paths to fake image folders\n","        processor: ViTImageProcessor\n","        batch_size: Batch size for evaluation\n","        model_name: Name of model for display\n","\n","    Returns:\n","        results: Dictionary with metrics and predictions\n","    \"\"\"\n","    print(\"=\"*70)\n","    print(f\"HOLD-OUT SET EVALUATION - {model_name}\")\n","    print(\"=\"*70)\n","\n","    # LABEL_REAL = 0, LABEL_FAKE = 1\n","    LABEL_REAL = 0\n","    LABEL_FAKE = 1\n","\n","    patterns_to_check = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n","\n","    # Collect all real images\n","    real_files = []\n","    for path in holdout_real_paths:\n","        for ext in patterns_to_check:\n","            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n","            real_files.extend(files)\n","        print(f\"Found {len([f for f in real_files if path in f])} REAL images in {path}\")\n","\n","    print(f\"Total REAL images: {len(real_files)}\")\n","\n","    # Collect all fake images\n","    fake_files = []\n","    for path in holdout_fake_paths:\n","        for ext in patterns_to_check:\n","            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n","            fake_files.extend(files)\n","        print(f\"Found {len([f for f in fake_files if path in f])} FAKE images in {path}\")\n","\n","    print(f\"Total FAKE images: {len(fake_files)}\")\n","\n","    # Combine files and labels\n","    all_files = real_files + fake_files\n","    all_labels = [LABEL_REAL] * len(real_files) + [LABEL_FAKE] * len(fake_files)\n","\n","    print(f\"\\nTotal hold-out images: {len(all_files)}\")\n","    print(f\"  REAL: {len(real_files)}\")\n","    print(f\"  FAKE: {len(fake_files)}\")\n","\n","    if len(all_files) == 0:\n","        print(\"Error: No images found!\")\n","        return None\n","\n","    # Create dataset\n","    holdout_dataset = KaggleImageDataset(\n","        file_paths=all_files,\n","        labels=all_labels,\n","        processor=processor,\n","        is_train=False  # No augmentation for testing\n","    )\n","\n","    # Create dataloader\n","    holdout_loader = DataLoader(\n","        holdout_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2\n","    )\n","\n","    # Run inference\n","    print(\"\\nRunning inference...\")\n","    device = next(model.parameters()).device\n","    model.eval()\n","\n","    all_preds = []\n","    all_probs = []\n","    all_labels_list = []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(holdout_loader, desc=\"Evaluating\"):\n","            pixel_values = batch['pixel_values'].to(device)\n","            labels_batch = batch['labels'].to(device)\n","\n","            outputs = model(pixel_values=pixel_values)\n","            logits = outputs.logits\n","            probs = torch.softmax(logits, dim=1)\n","            preds = torch.argmax(logits, dim=1)\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_probs.extend(probs.cpu().numpy())\n","            all_labels_list.extend(labels_batch.cpu().numpy())\n","\n","    all_preds = np.array(all_preds)\n","    all_probs = np.array(all_probs)\n","    all_labels_array = np.array(all_labels_list)\n","\n","    # Calculate metrics\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"RESULTS\")\n","    print(\"=\"*70)\n","\n","    accuracy = accuracy_score(all_labels_array, all_preds)\n","    print(f\"\\nOverall Accuracy: {accuracy*100:.2f}%\")\n","\n","    # Classification report\n","    print(\"\\n\" + classification_report(\n","        all_labels_array, all_preds,\n","        target_names=[\"REAL\", \"FAKE\"],\n","        digits=4\n","    ))\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(all_labels_array, all_preds)\n","    print(\"Confusion Matrix:\")\n","    print(\"                 Predicted\")\n","    print(\"               REAL    FAKE\")\n","    print(f\"Actual REAL   {cm[0][0]:5d}   {cm[0][1]:5d}\")\n","    print(f\"       FAKE   {cm[1][0]:5d}   {cm[1][1]:5d}\")\n","\n","    # Calculate per-class metrics\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        all_labels_array, all_preds, average=None, labels=[LABEL_REAL, LABEL_FAKE]\n","    )\n","\n","    print(f\"\\nPer-Class Metrics:\")\n","    print(f\"  REAL: Precision={precision[0]:.4f}, Recall={recall[0]:.4f}, F1={f1[0]:.4f}\")\n","    print(f\"  FAKE: Precision={precision[1]:.4f}, Recall={recall[1]:.4f}, F1={f1[1]:.4f}\")\n","\n","    # ROC-AUC\n","    try:\n","        auc = roc_auc_score(all_labels_array, all_probs[:, 1])\n","        print(f\"\\nROC-AUC Score: {auc:.4f}\")\n","    except:\n","        print(\"\\nROC-AUC Score: Could not calculate\")\n","        auc = None\n","\n","    # Per-source breakdown\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"PER-SOURCE BREAKDOWN\")\n","    print(\"=\"*70)\n","\n","    # Create source mapping\n","    sources = {}\n","    for path in holdout_real_paths:\n","        source_name = os.path.basename(path.rstrip('/'))\n","        sources[source_name] = {'pattern': path, 'type': 'REAL'}\n","\n","    for path in holdout_fake_paths:\n","        source_name = os.path.basename(path.rstrip('/'))\n","        sources[source_name] = {'pattern': path, 'type': 'FAKE'}\n","\n","    for source_name, source_info in sources.items():\n","        # Find indices for this source\n","        pattern = source_info['pattern']\n","        indices = [i for i, f in enumerate(all_files) if pattern in f]\n","\n","        if len(indices) == 0:\n","            continue\n","\n","        source_labels = all_labels_array[indices]\n","        source_preds = all_preds[indices]\n","\n","        source_acc = accuracy_score(source_labels, source_preds)\n","        correct = (source_labels == source_preds).sum()\n","\n","        print(f\"\\n--- {source_name} ({source_info['type']}) ---\")\n","        print(f\"Samples: {len(indices)}\")\n","        print(f\"Accuracy: {source_acc*100:.2f}% ({correct}/{len(indices)})\")\n","\n","        # Show breakdown if mixed labels\n","        unique_labels = np.unique(source_labels)\n","        if len(unique_labels) > 1:\n","            source_report = classification_report(\n","                source_labels, source_preds,\n","                target_names=[\"REAL\", \"FAKE\"],\n","                digits=4,\n","                zero_division=0\n","            )\n","            print(source_report)\n","\n","    print(\"=\"*70)\n","\n","    # Return results\n","    results = {\n","        'accuracy': accuracy,\n","        'predictions': all_preds,\n","        'probabilities': all_probs,\n","        'labels': all_labels_array,\n","        'file_paths': all_files,\n","        'confusion_matrix': cm,\n","        'auc': auc\n","    }\n","\n","    return results"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T23:00:23.112584Z","iopub.execute_input":"2025-11-21T23:00:23.113251Z","iopub.status.idle":"2025-11-21T23:00:23.129606Z","shell.execute_reply.started":"2025-11-21T23:00:23.113224Z","shell.execute_reply":"2025-11-21T23:00:23.128776Z"},"id":"NONu7-B0P3YK"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["processor = AutoImageProcessor.from_pretrained(model_name, use_fast = True)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T23:00:48.586011Z","iopub.execute_input":"2025-11-21T23:00:48.586286Z","iopub.status.idle":"2025-11-21T23:00:48.949559Z","shell.execute_reply.started":"2025-11-21T23:00:48.586265Z","shell.execute_reply":"2025-11-21T23:00:48.948883Z"},"colab":{"referenced_widgets":["c08407137e5a4a22ac221e51ee642acb","81a911d9e9c24a748e44f0d1e64d65ac"]},"id":"PZ63eqvmP3YK","outputId":"80a72bd6-135d-4c24-d0b9-d4f46c68f364"},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c08407137e5a4a22ac221e51ee642acb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a911d9e9c24a748e44f0d1e64d65ac"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":["best_model = ViTForImageClassification.from_pretrained(\"/kaggle/input/vit-trained-model/checkpoint-17030\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T23:00:50.613763Z","iopub.execute_input":"2025-11-21T23:00:50.614496Z","iopub.status.idle":"2025-11-21T23:00:51.446385Z","shell.execute_reply.started":"2025-11-21T23:00:50.614471Z","shell.execute_reply":"2025-11-21T23:00:51.445215Z"},"id":"UWKTVRLoP3YK"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","best_model.to(device)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T23:05:28.677191Z","iopub.execute_input":"2025-11-21T23:05:28.677957Z","iopub.status.idle":"2025-11-21T23:05:28.933668Z","shell.execute_reply.started":"2025-11-21T23:05:28.677911Z","shell.execute_reply":"2025-11-21T23:05:28.932963Z"},"id":"NZaDt7KxP3YK","outputId":"4de0af4a-2d55-4a76-d671-3869d5224d0f"},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"ViTForImageClassification(\n  (vit): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":["holdout_real_paths = [\n","    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/test/real\",\n","    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/train/real\",\n","    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/val/real\"\n","]\n","\n","holdout_fake_paths = [\n","    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/test/fake\",\n","    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/train/fake\",\n","    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/val/fake\"\n","]"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T23:05:35.615412Z","iopub.execute_input":"2025-11-21T23:05:35.616074Z","iopub.status.idle":"2025-11-21T23:05:35.621076Z","shell.execute_reply.started":"2025-11-21T23:05:35.616034Z","shell.execute_reply":"2025-11-21T23:05:35.620268Z"},"id":"pMHoL_LbP3YK"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["results = evaluate_holdout_set(\n","    model = best_model,\n","    holdout_real_paths = holdout_real_paths,\n","    holdout_fake_paths = holdout_fake_paths,\n","    processor = processor,\n","    batch_size = 32,\n","    model_name = \"ViT\"\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T23:05:36.459257Z","iopub.execute_input":"2025-11-21T23:05:36.459529Z","iopub.status.idle":"2025-11-21T23:10:43.038396Z","shell.execute_reply.started":"2025-11-21T23:05:36.459509Z","shell.execute_reply":"2025-11-21T23:10:43.037480Z"},"id":"RdcbSknkP3YK","outputId":"3baadf18-1e25-4ba9-90fa-77c04bb0b6a9"},"outputs":[{"name":"stdout","text":"======================================================================\nHOLD-OUT SET EVALUATION - ViT\n======================================================================\nFound 1606 REAL images in /kaggle/input/deepfake-hold-out-testing/hold-out-testing/test/real\nFound 12848 REAL images in /kaggle/input/deepfake-hold-out-testing/hold-out-testing/train/real\nFound 1606 REAL images in /kaggle/input/deepfake-hold-out-testing/hold-out-testing/val/real\nTotal REAL images: 16060\nFound 1606 FAKE images in /kaggle/input/deepfake-hold-out-testing/hold-out-testing/test/fake\nFound 12848 FAKE images in /kaggle/input/deepfake-hold-out-testing/hold-out-testing/train/fake\nFound 1606 FAKE images in /kaggle/input/deepfake-hold-out-testing/hold-out-testing/val/fake\nTotal FAKE images: 16060\n\nTotal hold-out images: 32120\n  REAL: 16060\n  FAKE: 16060\n\nRunning inference...\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 1004/1004 [04:58<00:00,  3.37it/s]","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\nRESULTS\n======================================================================\n\nOverall Accuracy: 58.14%\n\n              precision    recall  f1-score   support\n\n        REAL     0.5498    0.8980    0.6821     16060\n        FAKE     0.7219    0.2648    0.3874     16060\n\n    accuracy                         0.5814     32120\n   macro avg     0.6359    0.5814    0.5347     32120\nweighted avg     0.6359    0.5814    0.5347     32120\n\nConfusion Matrix:\n                 Predicted\n               REAL    FAKE\nActual REAL   14422    1638\n       FAKE   11808    4252\n\nPer-Class Metrics:\n  REAL: Precision=0.5498, Recall=0.8980, F1=0.6821\n  FAKE: Precision=0.7219, Recall=0.2648, F1=0.3874\n\nROC-AUC Score: 0.5808\n\n======================================================================\nPER-SOURCE BREAKDOWN\n======================================================================\n\n--- real (REAL) ---\nSamples: 1606\nAccuracy: 87.36% (1403/1606)\n\n--- fake (FAKE) ---\nSamples: 1606\nAccuracy: 16.06% (258/1606)\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["if results:\n","    print(f\"\\nFinal Accuracy: {results['accuracy']*100:.2f}%\")\n","    if results['auc']:\n","        print(f\"AUC: {results['auc']:.4f}\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T23:11:39.069744Z","iopub.execute_input":"2025-11-21T23:11:39.070218Z","iopub.status.idle":"2025-11-21T23:11:39.074392Z","shell.execute_reply.started":"2025-11-21T23:11:39.070193Z","shell.execute_reply":"2025-11-21T23:11:39.073727Z"},"id":"Jt7Bct_vP3YK","outputId":"95ad04ae-767e-4a7b-e459-f9986c9db5b2"},"outputs":[{"name":"stdout","text":"\nFinal Accuracy: 58.14%\nAUC: 0.5808\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":[],"metadata":{"trusted":true,"id":"9jhZ2H7_P3YK"},"outputs":[],"execution_count":null}]}