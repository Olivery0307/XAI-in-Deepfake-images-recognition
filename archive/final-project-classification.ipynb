{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO5FnImoFEWY"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:07.130613Z",
          "iopub.status.busy": "2025-11-24T20:24:07.130372Z",
          "iopub.status.idle": "2025-11-24T20:24:40.747246Z",
          "shell.execute_reply": "2025-11-24T20:24:40.746693Z",
          "shell.execute_reply.started": "2025-11-24T20:24:07.130590Z"
        },
        "executionInfo": {
          "elapsed": 63210,
          "status": "ok",
          "timestamp": 1762829608417,
          "user": {
            "displayName": "LIANG-JIE CHIU",
            "userId": "03072225676804136983"
          },
          "user_tz": 300
        },
        "id": "mRdgDUABFyeF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import ViTForImageClassification, TrainingArguments, Trainer, ViTImageProcessor, EarlyStoppingCallback, AutoImageProcessor, SwinForImageClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix, roc_auc_score\n",
        "from PIL import Image, ImageFilter\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import sys\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import glob\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbE3cpAmFnZ5"
      },
      "source": [
        "# Image Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:40.748742Z",
          "iopub.status.busy": "2025-11-24T20:24:40.747938Z",
          "iopub.status.idle": "2025-11-24T20:24:40.756530Z",
          "shell.execute_reply": "2025-11-24T20:24:40.755870Z",
          "shell.execute_reply.started": "2025-11-24T20:24:40.748722Z"
        },
        "id": "1aD4MTCrFqqu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class KaggleImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset to load images directly from the local file system\n",
        "    for use with HuggingFace Trainer, with optional augmentations.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_paths, labels, processor, is_train=False):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "        self.is_train = is_train  \n",
        "        \n",
        "        # Define augmentations for training\n",
        "        if self.is_train:\n",
        "            self.augmentations = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomApply([\n",
        "                    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n",
        "                ], p=0.3),\n",
        "                transforms.RandomApply([\n",
        "                    transforms.GaussianBlur(kernel_size=3)\n",
        "                ], p=0.3),\n",
        "                transforms.RandomRotation(degrees=10),\n",
        "            ])\n",
        "        else:\n",
        "            self.augmentations = None\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples.\"\"\"\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetches the image from local path, applies augmentations and processor, \n",
        "        and returns the sample in HuggingFace format.\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Get the local path and label\n",
        "        local_path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        try:\n",
        "            # Open the file from local path\n",
        "            image = Image.open(local_path).convert('RGB')\n",
        "\n",
        "            # Apply augmentations BEFORE the processor (only for training)\n",
        "            if self.augmentations is not None:\n",
        "                image = self.augmentations(image)\n",
        "\n",
        "            # Use the ViT processor (handles resizing and normalization)\n",
        "            processed = self.processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "            # Extract the pixel values and remove the batch dimension\n",
        "            pixel_values = processed['pixel_values'].squeeze(0)\n",
        "\n",
        "            # Return in HuggingFace format\n",
        "            return {\n",
        "                'pixel_values': pixel_values,\n",
        "                'labels': torch.tensor(label, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {local_path}: {e}\")\n",
        "            # Return a dummy sample if loading fails\n",
        "            dummy_image = Image.new('RGB', (224, 224), color='black')\n",
        "            processed = self.processor(images=dummy_image, return_tensors=\"pt\")\n",
        "            return {\n",
        "                'pixel_values': processed['pixel_values'].squeeze(0),\n",
        "                'labels': torch.tensor(0, dtype=torch.long)\n",
        "            }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:40.759024Z",
          "iopub.status.busy": "2025-11-24T20:24:40.758677Z",
          "iopub.status.idle": "2025-11-24T20:24:40.826244Z",
          "shell.execute_reply": "2025-11-24T20:24:40.825489Z",
          "shell.execute_reply.started": "2025-11-24T20:24:40.758999Z"
        },
        "id": "socv-cDOJv19",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_data_mixed_structure(video_real_paths, video_fake_paths, \n",
        "                              image_real_paths, image_fake_paths, \n",
        "                              model_name, random_seed):\n",
        "    \"\"\"\n",
        "    Scans local directories and handles two types of datasets:\n",
        "    1. Video-based: Split BY FOLDER to prevent frame leakage\n",
        "    2. Image-based: Split BY IMAGE (no folder structure)\n",
        "    \n",
        "    Args:\n",
        "        video_real_paths: List of paths to real video folders (e.g., Celeb-real, YouTube-real)\n",
        "        video_fake_paths: List of paths to fake video folders (e.g., Celeb-synthesis)\n",
        "        image_real_paths: List of paths to real image folders (e.g., FFHQ-real-v2)\n",
        "        image_fake_paths: List of paths to fake image folders (e.g., StableDiffusion-fake-v2, stylegan-6000)\n",
        "        model_name: HuggingFace model name for processor\n",
        "        random_seed: Random seed for reproducibility\n",
        "    \"\"\"\n",
        "    \n",
        "    patterns_to_check = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n",
        "    \n",
        "    # ========================================\n",
        "    # PART 1: Handle VIDEO-BASED datasets (split by folder)\n",
        "    # ========================================\n",
        "    real_video_folders = defaultdict(list)\n",
        "    fake_video_folders = defaultdict(list)\n",
        "    \n",
        "    # Get REAL video folders\n",
        "    for path in video_real_paths:\n",
        "        for ext in patterns_to_check:\n",
        "            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n",
        "            for file in files:\n",
        "                parent_folder = os.path.dirname(file)\n",
        "                real_video_folders[parent_folder].append(file)\n",
        "        \n",
        "        print(f\"Found {len(real_video_folders)} REAL video folders in {path}\")\n",
        "        total_files = sum(len(files) for files in real_video_folders.values())\n",
        "        print(f\"  Total REAL video frames: {total_files}\")\n",
        "    \n",
        "    # Get FAKE video folders\n",
        "    for path in video_fake_paths:\n",
        "        for ext in patterns_to_check:\n",
        "            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n",
        "            for file in files:\n",
        "                parent_folder = os.path.dirname(file)\n",
        "                fake_video_folders[parent_folder].append(file)\n",
        "        \n",
        "        print(f\"Found {len(fake_video_folders)} FAKE video folders in {path}\")\n",
        "        total_files = sum(len(files) for files in fake_video_folders.values())\n",
        "        print(f\"  Total FAKE video frames: {total_files}\")\n",
        "    \n",
        "    # Split video folders (70/15/15)\n",
        "    train_real_video_folders, val_real_video_folders, test_real_video_folders = [], [], []\n",
        "    train_fake_video_folders, val_fake_video_folders, test_fake_video_folders = [], [], []\n",
        "    \n",
        "    if len(real_video_folders) > 0:\n",
        "        real_folder_names = list(real_video_folders.keys())\n",
        "        train_real_video_folders, temp_real = train_test_split(\n",
        "            real_folder_names, test_size=0.3, random_state=random_seed\n",
        "        )\n",
        "        val_real_video_folders, test_real_video_folders = train_test_split(\n",
        "            temp_real, test_size=0.5, random_state=random_seed\n",
        "        )\n",
        "    \n",
        "    if len(fake_video_folders) > 0:\n",
        "        fake_folder_names = list(fake_video_folders.keys())\n",
        "        train_fake_video_folders, temp_fake = train_test_split(\n",
        "            fake_folder_names, test_size=0.3, random_state=random_seed\n",
        "        )\n",
        "        val_fake_video_folders, test_fake_video_folders = train_test_split(\n",
        "            temp_fake, test_size=0.5, random_state=random_seed\n",
        "        )\n",
        "    \n",
        "    # ========================================\n",
        "    # PART 2: Handle IMAGE-BASED datasets (split by image)\n",
        "    # ========================================\n",
        "    real_image_files = []\n",
        "    fake_image_files = []\n",
        "    \n",
        "    # Get REAL image files\n",
        "    for path in image_real_paths:\n",
        "        for ext in patterns_to_check:\n",
        "            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n",
        "            real_image_files.extend(files)\n",
        "        print(f\"Found {len([f for f in real_image_files if path in f])} REAL images in {path}\")\n",
        "    \n",
        "    print(f\"  Total REAL images: {len(real_image_files)}\")\n",
        "    \n",
        "    # Get FAKE image files\n",
        "    for path in image_fake_paths:\n",
        "        for ext in patterns_to_check:\n",
        "            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n",
        "            fake_image_files.extend(files)\n",
        "        print(f\"Found {len([f for f in fake_image_files if path in f])} FAKE images in {path}\")\n",
        "    \n",
        "    print(f\"  Total FAKE images: {len(fake_image_files)}\")\n",
        "    \n",
        "    # Split image files (70/15/15)\n",
        "    train_real_images, val_real_images, test_real_images = [], [], []\n",
        "    train_fake_images, val_fake_images, test_fake_images = [], [], []\n",
        "    \n",
        "    if len(real_image_files) > 0:\n",
        "        train_real_images, temp_real = train_test_split(\n",
        "            real_image_files, test_size=0.3, random_state=random_seed\n",
        "        )\n",
        "        val_real_images, test_real_images = train_test_split(\n",
        "            temp_real, test_size=0.5, random_state=random_seed\n",
        "        )\n",
        "    \n",
        "    if len(fake_image_files) > 0:\n",
        "        train_fake_images, temp_fake = train_test_split(\n",
        "            fake_image_files, test_size=0.3, random_state=random_seed\n",
        "        )\n",
        "        val_fake_images, test_fake_images = train_test_split(\n",
        "            temp_fake, test_size=0.5, random_state=random_seed\n",
        "        )\n",
        "    \n",
        "    # ========================================\n",
        "    # PART 3: Combine video-based and image-based data\n",
        "    # ========================================\n",
        "    train_files, train_labels = [], []\n",
        "    val_files, val_labels = [], []\n",
        "    test_files, test_labels = [], []\n",
        "    \n",
        "    # Add REAL VIDEO frames to splits\n",
        "    for folder in train_real_video_folders:\n",
        "        train_files.extend(real_video_folders[folder])\n",
        "        train_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n",
        "    \n",
        "    for folder in val_real_video_folders:\n",
        "        val_files.extend(real_video_folders[folder])\n",
        "        val_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n",
        "    \n",
        "    for folder in test_real_video_folders:\n",
        "        test_files.extend(real_video_folders[folder])\n",
        "        test_labels.extend([LABEL_REAL] * len(real_video_folders[folder]))\n",
        "    \n",
        "    # Add FAKE VIDEO frames to splits\n",
        "    for folder in train_fake_video_folders:\n",
        "        train_files.extend(fake_video_folders[folder])\n",
        "        train_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n",
        "    \n",
        "    for folder in val_fake_video_folders:\n",
        "        val_files.extend(fake_video_folders[folder])\n",
        "        val_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n",
        "    \n",
        "    for folder in test_fake_video_folders:\n",
        "        test_files.extend(fake_video_folders[folder])\n",
        "        test_labels.extend([LABEL_FAKE] * len(fake_video_folders[folder]))\n",
        "    \n",
        "    # Add REAL IMAGES to splits\n",
        "    train_files.extend(train_real_images)\n",
        "    train_labels.extend([LABEL_REAL] * len(train_real_images))\n",
        "    \n",
        "    val_files.extend(val_real_images)\n",
        "    val_labels.extend([LABEL_REAL] * len(val_real_images))\n",
        "    \n",
        "    test_files.extend(test_real_images)\n",
        "    test_labels.extend([LABEL_REAL] * len(test_real_images))\n",
        "    \n",
        "    # Add FAKE IMAGES to splits\n",
        "    train_files.extend(train_fake_images)\n",
        "    train_labels.extend([LABEL_FAKE] * len(train_fake_images))\n",
        "    \n",
        "    val_files.extend(val_fake_images)\n",
        "    val_labels.extend([LABEL_FAKE] * len(val_fake_images))\n",
        "    \n",
        "    test_files.extend(test_fake_images)\n",
        "    test_labels.extend([LABEL_FAKE] * len(test_fake_images))\n",
        "    \n",
        "    # ========================================\n",
        "    # PART 4: Print detailed statistics\n",
        "    # ========================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"MIXED DATASET STATISTICS (Video-based + Image-based)\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    print(\"\\n--- VIDEO-BASED DATA (split by folder) ---\")\n",
        "    if len(real_video_folders) > 0:\n",
        "        print(f\"Real video folders: {len(real_video_folders)} total\")\n",
        "        print(f\"  Train: {len(train_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in train_real_video_folders)} frames\")\n",
        "        print(f\"  Val:   {len(val_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in val_real_video_folders)} frames\")\n",
        "        print(f\"  Test:  {len(test_real_video_folders)} folders, {sum(len(real_video_folders[f]) for f in test_real_video_folders)} frames\")\n",
        "    else:\n",
        "        print(\"No real video data\")\n",
        "    \n",
        "    if len(fake_video_folders) > 0:\n",
        "        print(f\"\\nFake video folders: {len(fake_video_folders)} total\")\n",
        "        print(f\"  Train: {len(train_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in train_fake_video_folders)} frames\")\n",
        "        print(f\"  Val:   {len(val_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in val_fake_video_folders)} frames\")\n",
        "        print(f\"  Test:  {len(test_fake_video_folders)} folders, {sum(len(fake_video_folders[f]) for f in test_fake_video_folders)} frames\")\n",
        "    else:\n",
        "        print(\"No fake video data\")\n",
        "    \n",
        "    print(\"\\n--- IMAGE-BASED DATA (split by image) ---\")\n",
        "    if len(real_image_files) > 0:\n",
        "        print(f\"Real images: {len(real_image_files)} total\")\n",
        "        print(f\"  Train: {len(train_real_images)} images\")\n",
        "        print(f\"  Val:   {len(val_real_images)} images\")\n",
        "        print(f\"  Test:  {len(test_real_images)} images\")\n",
        "    else:\n",
        "        print(\"No real image data\")\n",
        "    \n",
        "    if len(fake_image_files) > 0:\n",
        "        print(f\"\\nFake images: {len(fake_image_files)} total\")\n",
        "        print(f\"  Train: {len(train_fake_images)} images\")\n",
        "        print(f\"  Val:   {len(val_fake_images)} images\")\n",
        "        print(f\"  Test:  {len(test_fake_images)} images\")\n",
        "    else:\n",
        "        print(\"No fake image data\")\n",
        "    \n",
        "    print(\"\\n--- COMBINED TOTALS ---\")\n",
        "    print(f\"Train: {len(train_files)} total ({train_labels.count(LABEL_REAL)} real, {train_labels.count(LABEL_FAKE)} fake)\")\n",
        "    print(f\"Val:   {len(val_files)} total ({val_labels.count(LABEL_REAL)} real, {val_labels.count(LABEL_FAKE)} fake)\")\n",
        "    print(f\"Test:  {len(test_files)} total ({test_labels.count(LABEL_REAL)} real, {test_labels.count(LABEL_FAKE)} fake)\")\n",
        "    print(f\"\\nGrand Total: {len(train_files) + len(val_files) + len(test_files)} images\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # ========================================\n",
        "    # PART 5: Create datasets\n",
        "    # ========================================\n",
        "    processor = AutoImageProcessor.from_pretrained(model_name, use_fast = True)\n",
        "    \n",
        "    train_dataset = KaggleImageDataset(\n",
        "        file_paths=train_files,\n",
        "        labels=train_labels,\n",
        "        processor=processor,\n",
        "        is_train=True\n",
        "    )\n",
        "    val_dataset = KaggleImageDataset(\n",
        "        file_paths=val_files,\n",
        "        labels=val_labels,\n",
        "        processor=processor,\n",
        "        is_train=False\n",
        "    )\n",
        "    test_dataset = KaggleImageDataset(\n",
        "        file_paths=test_files,\n",
        "        labels=test_labels,\n",
        "        processor=processor,\n",
        "        is_train=False\n",
        "    )\n",
        "    \n",
        "    return train_dataset, val_dataset, test_dataset, processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:40.827123Z",
          "iopub.status.busy": "2025-11-24T20:24:40.826921Z",
          "iopub.status.idle": "2025-11-24T20:24:40.843991Z",
          "shell.execute_reply": "2025-11-24T20:24:40.843289Z",
          "shell.execute_reply.started": "2025-11-24T20:24:40.827099Z"
        },
        "id": "TORZeyN6Qpx_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    \n",
        "    # Already converted to predictions, not logits\n",
        "    if isinstance(predictions, torch.Tensor):\n",
        "        predictions = predictions.cpu().numpy()\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.cpu().numpy()\n",
        "    \n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, predictions, average='binary'\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:40.847358Z",
          "iopub.status.busy": "2025-11-24T20:24:40.847130Z",
          "iopub.status.idle": "2025-11-24T20:24:40.859401Z",
          "shell.execute_reply": "2025-11-24T20:24:40.858795Z",
          "shell.execute_reply.started": "2025-11-24T20:24:40.847338Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class MemoryEfficientTrainer(Trainer):\n",
        "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
        "        \"\"\"\n",
        "        Override to return predictions instead of full logits\n",
        "        \"\"\"\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "        \n",
        "        # Return predictions instead of logits to save memory\n",
        "        if prediction_loss_only:\n",
        "            return (loss, None, None)\n",
        "        \n",
        "        # Convert to predictions immediately\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        labels = inputs.get(\"labels\")\n",
        "        \n",
        "        return (loss, preds, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:40.860285Z",
          "iopub.status.busy": "2025-11-24T20:24:40.860031Z",
          "iopub.status.idle": "2025-11-24T20:24:40.873288Z",
          "shell.execute_reply": "2025-11-24T20:24:40.872676Z",
          "shell.execute_reply.started": "2025-11-24T20:24:40.860262Z"
        },
        "id": "MZdo0m85x9UE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def attetion_rollout(attentions, discard_ratio=0.9):\n",
        "    \"\"\"\n",
        "    Compute attention rollout from all transformer layers.\n",
        "    Args:\n",
        "        attentions: tuple of attention tensors from each layer\n",
        "        discard_ratio: percentage of lowest attention values to discard\n",
        "    Returns:\n",
        "        Attention map for the [CLS] token\n",
        "    \"\"\"\n",
        "    # Get device from first attention tensor\n",
        "    device = attentions[0].device\n",
        "    \n",
        "    # Create identity matrix on the same device\n",
        "    result = torch.eye(attentions[0].size(-1)).to(device)\n",
        "    \n",
        "    for attention in attentions:\n",
        "        # Average across all heads\n",
        "        attention_heads_fused = attention.mean(dim=1)\n",
        "        attention_heads_fused = attention_heads_fused[0]\n",
        "        \n",
        "        # Drop the lowest attentions\n",
        "        flat = attention_heads_fused.view(-1)\n",
        "        _, indices = flat.topk(k=int(flat.size(-1) * discard_ratio), largest=False)\n",
        "        flat[indices] = 0\n",
        "        \n",
        "        # Normalize\n",
        "        I = torch.eye(attention_heads_fused.size(-1)).to(device)  # Fix: add .to(device)\n",
        "        a = (attention_heads_fused + 1.0 * I) / 2\n",
        "        a = a / a.sum(dim=-1, keepdim=True)\n",
        "        result = torch.matmul(a, result)\n",
        "    \n",
        "    mask = result[0, 1:]\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:40.874086Z",
          "iopub.status.busy": "2025-11-24T20:24:40.873895Z",
          "iopub.status.idle": "2025-11-24T20:24:40.893004Z",
          "shell.execute_reply": "2025-11-24T20:24:40.892464Z",
          "shell.execute_reply.started": "2025-11-24T20:24:40.874061Z"
        },
        "id": "SW-RbGL81XRQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def visualize_attention(model, image_path, processor, true_label=None):\n",
        "    \"\"\"\n",
        "    Visualize attention rollout for a single image.\n",
        "    \n",
        "    Args:\n",
        "        model: ViT model with output_attentions=True\n",
        "        image_path: Path to local image file\n",
        "        processor: ViTImageProcessor for preprocessing\n",
        "        true_label: Optional true label (0 for FAKE, 1 for REAL, or string)\n",
        "    \"\"\"\n",
        "    # Load image from local path\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    \n",
        "    # Process image\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    \n",
        "    # Move inputs to same device as model\n",
        "    device = next(model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Get model outputs with attentions\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, output_attentions=True)\n",
        "    \n",
        "    # Get attention weights\n",
        "    attentions = outputs.attentions  # tuple of (num_layers) tensors\n",
        "    \n",
        "    # Compute attention rollout\n",
        "    mask = attetion_rollout(attentions)\n",
        "    \n",
        "    # Reshape mask to image dimensions\n",
        "    num_patches = int(mask.shape[0] ** 0.5)\n",
        "    mask = mask.reshape(num_patches, num_patches).cpu().numpy()\n",
        "    \n",
        "    # Resize to original image size\n",
        "    mask = Image.fromarray((mask * 255).astype(np.uint8)).resize(\n",
        "        image.size, resample=Image.BILINEAR\n",
        "    )\n",
        "    mask = np.array(mask) / 255.0\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    \n",
        "    # Original image\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title('Original Image')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Attention heatmap\n",
        "    axes[1].imshow(mask, cmap='jet')\n",
        "    axes[1].set_title('Attention Rollout')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    # Overlay\n",
        "    axes[2].imshow(image)\n",
        "    axes[2].imshow(mask, cmap='jet', alpha=0.5)\n",
        "    axes[2].set_title('Overlay')\n",
        "    axes[2].axis('off')\n",
        "    \n",
        "    # Get prediction\n",
        "    prediction = outputs.logits.argmax(-1).item()\n",
        "    pred_label = model.config.id2label[prediction]\n",
        "    prob = torch.softmax(outputs.logits, dim=-1)[0][prediction].item()\n",
        "    \n",
        "    # Build title with prediction and true label\n",
        "    title_parts = [f'Prediction: {pred_label} ({prob:.2%})']\n",
        "    \n",
        "    if true_label is not None:\n",
        "        # Convert true_label to string if it's numeric\n",
        "        if isinstance(true_label, (int, np.integer)):\n",
        "            true_label_str = model.config.id2label[true_label]\n",
        "        else:\n",
        "            true_label_str = true_label\n",
        "        \n",
        "        # Check if prediction is correct\n",
        "        is_correct = (pred_label == true_label_str)\n",
        "        correctness = \"✓\" if is_correct else \"✗\"\n",
        "        \n",
        "        title_parts.append(f'True Label: {true_label_str} {correctness}')\n",
        "    \n",
        "    fig.suptitle(' | '.join(title_parts), fontsize=16)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:40.895231Z",
          "iopub.status.busy": "2025-11-24T20:24:40.895036Z",
          "iopub.status.idle": "2025-11-24T20:24:40.910898Z",
          "shell.execute_reply": "2025-11-24T20:24:40.910251Z",
          "shell.execute_reply.started": "2025-11-24T20:24:40.895217Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def visualize_attention_per_domain(model, test_dataset, processor, n_samples=10):\n",
        "    \"\"\"\n",
        "    Visualize attention rollout for random samples from each domain.\n",
        "    \n",
        "    Args:\n",
        "        model: ViT model with output_attentions=True\n",
        "        test_dataset: Test dataset\n",
        "        processor: ViTImageProcessor\n",
        "        n_samples: Number of samples to visualize per domain\n",
        "        random_seed: Random seed for reproducibility\n",
        "    \"\"\"\n",
        "    # Define domains\n",
        "    domains = {\n",
        "        'Celeb-real (video)': 'Celeb-real',\n",
        "        'YouTube-real (video)': 'YouTube-real',\n",
        "        'Celeb-synthesis (video)': 'Celeb-synthesis',\n",
        "        'FFHQ-real (image)': 'FFHQ-real-v2',\n",
        "        'StableDiffusion-fake (image)': 'StableDiffusion-fake-v2',\n",
        "        'StyleGAN-fake (image)': 'stylegan',\n",
        "    }\n",
        "    \n",
        "    paths = test_dataset.file_paths\n",
        "    labels = test_dataset.labels\n",
        "    \n",
        "    # Process each domain\n",
        "    for domain_name, pattern in domains.items():\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"VISUALIZING: {domain_name}\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # Find indices for this domain\n",
        "        domain_indices = [i for i, p in enumerate(paths) if pattern in p]\n",
        "        \n",
        "        if len(domain_indices) == 0:\n",
        "            print(f\"No samples found for {domain_name}\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"Total samples in domain: {len(domain_indices)}\")\n",
        "        \n",
        "        # Sample random indices\n",
        "        n_to_sample = min(n_samples, len(domain_indices))\n",
        "        sampled_indices = np.random.choice(domain_indices, size=n_to_sample, replace=False)\n",
        "        \n",
        "        print(f\"Visualizing {n_to_sample} random samples...\\n\")\n",
        "        \n",
        "        # Visualize each sample\n",
        "        for idx in sampled_indices:\n",
        "            image_path = paths[idx]\n",
        "            true_label = labels[idx]\n",
        "            \n",
        "            print(f\"Sample {idx}: {image_path.split('/')[-1]}\")\n",
        "            visualize_attention(model, image_path, processor, true_label=true_label)\n",
        "            print()  # Add spacing between visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:40.911771Z",
          "iopub.status.busy": "2025-11-24T20:24:40.911498Z",
          "iopub.status.idle": "2025-11-24T20:24:40.929070Z",
          "shell.execute_reply": "2025-11-24T20:24:40.928381Z",
          "shell.execute_reply.started": "2025-11-24T20:24:40.911748Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def evaluate_per_domain(trainer, test_dataset):\n",
        "    \"\"\"\n",
        "    Simpler version using trainer.predict()\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import classification_report\n",
        "    \n",
        "    # Get predictions\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    preds = predictions.predictions  \n",
        "    labels = predictions.label_ids     # Shape: (n_samples,)\n",
        "    paths = test_dataset.file_paths\n",
        "    \n",
        "    # Overall report\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"OVERALL RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    print(classification_report(labels, preds, target_names=[\"FAKE\", \"REAL\"], digits=4))\n",
        "    \n",
        "    # Per-domain analysis\n",
        "    domains = {\n",
        "        'Celeb-real': 'Celeb-real',\n",
        "        'YouTube-real': 'YouTube-real',\n",
        "        'Celeb-synthesis': 'Celeb-synthesis',\n",
        "        'FFHQ-real': 'FFHQ-real',\n",
        "        'StableDiffusion-fake': 'StableDiffusion-fake-v2',\n",
        "        'StyleGAN-fake': 'stylegan',\n",
        "    }\n",
        "    \n",
        "    for domain_name, pattern in domains.items():\n",
        "        indices = [i for i, p in enumerate(paths) if pattern in p]\n",
        "        if not indices:\n",
        "            continue\n",
        "            \n",
        "        domain_labels = [labels[i] for i in indices]\n",
        "        domain_preds = [preds[i] for i in indices]\n",
        "        \n",
        "        print(f\"\\n--- {domain_name} ({len(indices)} samples) ---\")\n",
        "        if len(set(domain_labels)) > 1:\n",
        "            print(classification_report(domain_labels, domain_preds, \n",
        "                                       target_names=[\"FAKE\", \"REAL\"], digits=4))\n",
        "        else:\n",
        "            acc = sum(1 for i in range(len(domain_labels)) if domain_labels[i] == domain_preds[i])\n",
        "            print(f\"Accuracy: {acc/len(domain_labels)*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:53.864528Z",
          "iopub.status.busy": "2025-11-24T20:24:53.864248Z",
          "iopub.status.idle": "2025-11-24T20:24:53.869214Z",
          "shell.execute_reply": "2025-11-24T20:24:53.868505Z",
          "shell.execute_reply.started": "2025-11-24T20:24:53.864509Z"
        },
        "id": "LsCKdndrS27Q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:55.607234Z",
          "iopub.status.busy": "2025-11-24T20:24:55.606444Z",
          "iopub.status.idle": "2025-11-24T20:24:55.612703Z",
          "shell.execute_reply": "2025-11-24T20:24:55.611712Z",
          "shell.execute_reply.started": "2025-11-24T20:24:55.607203Z"
        },
        "id": "cuAo95zcRL5V",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define Parameters and Input\n",
        "video_real_paths = [\n",
        "    \"/kaggle/input/deepfake-images/Celeb-DF/data/Celeb-real\",\n",
        "    \"/kaggle/input/deepfake-images/Celeb-DF/data/YouTube-real\"\n",
        "]\n",
        "video_fake_paths = [\n",
        "    \"/kaggle/input/deepfake-images/Celeb-DF/data/Celeb-synthesis\"\n",
        "]\n",
        "\n",
        "image_real_paths = [\n",
        "    \"/kaggle/input/deepfake-images/FFHQ-real-v2/FFHQ-real-v2\"\n",
        "]\n",
        "\n",
        "image_fake_paths = [\n",
        "    \"/kaggle/input/deepfake-images/StableDiffusion-fake-v2/StableDiffusion-fake-v2\",\n",
        "    \"/kaggle/input/stylegan-6000/kaggle/working/stylegan_fake_dataset_nvidia\"\n",
        "]\n",
        "\n",
        "LABEL_REAL = 0\n",
        "LABEL_FAKE = 1\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 16\n",
        "RANDOM_SEED = 42\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "model_name = \"google/vit-base-patch16-224\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:57.968606Z",
          "iopub.status.busy": "2025-11-24T20:24:57.967827Z",
          "iopub.status.idle": "2025-11-24T20:24:57.975805Z",
          "shell.execute_reply": "2025-11-24T20:24:57.975004Z",
          "shell.execute_reply.started": "2025-11-24T20:24:57.968555Z"
        },
        "id": "3SyLZKpcS1MK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "seed_everything(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:24:59.281043Z",
          "iopub.status.busy": "2025-11-24T20:24:59.280787Z",
          "iopub.status.idle": "2025-11-24T20:26:11.835041Z",
          "shell.execute_reply": "2025-11-24T20:26:11.834323Z",
          "shell.execute_reply.started": "2025-11-24T20:24:59.281026Z"
        },
        "id": "vBGXsxCVKdR9",
        "outputId": "2a119f29-d678-46db-e551-584db4dc1f1d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset, processor = get_data_mixed_structure(\n",
        "    video_real_paths = video_real_paths,\n",
        "    video_fake_paths = video_fake_paths,\n",
        "    image_real_paths = image_real_paths,\n",
        "    image_fake_paths = image_fake_paths,\n",
        "    model_name = model_name,\n",
        "    random_seed = RANDOM_SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-24T20:26:16.903310Z",
          "iopub.status.busy": "2025-11-24T20:26:16.902706Z",
          "iopub.status.idle": "2025-11-24T20:26:16.907866Z",
          "shell.execute_reply": "2025-11-24T20:26:16.907067Z",
          "shell.execute_reply.started": "2025-11-24T20:26:16.903278Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T17:20:50.791581Z",
          "iopub.status.busy": "2025-11-15T17:20:50.791324Z",
          "iopub.status.idle": "2025-11-15T17:20:53.513636Z",
          "shell.execute_reply": "2025-11-15T17:20:53.513034Z",
          "shell.execute_reply.started": "2025-11-15T17:20:50.791560Z"
        },
        "id": "VMrtMHdRRXSJ",
        "outputId": "c4c59d37-9f29-4901-8ff4-0e1a6815839e",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels = 2,\n",
        "    id2label = {0: \"REAL\", 1: \"FAKE\"},\n",
        "    label2id = {\"REAL\": 0, \"FAKE\": 1},\n",
        "    ignore_mismatched_sizes = True,\n",
        "    output_attentions = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T17:20:53.515518Z",
          "iopub.status.busy": "2025-11-15T17:20:53.515302Z",
          "iopub.status.idle": "2025-11-15T17:20:53.520629Z",
          "shell.execute_reply": "2025-11-15T17:20:53.519890Z",
          "shell.execute_reply.started": "2025-11-15T17:20:53.515502Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for param in model.vit.embeddings.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Freeze all encoder layers except the last 2\n",
        "num_layers = len(model.vit.encoder.layer)\n",
        "for i, layer in enumerate(model.vit.encoder.layer):\n",
        "    if i < num_layers - 2:  # Freeze all but last 2 layers\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "for param in model.vit.layernorm.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T17:20:53.521780Z",
          "iopub.status.busy": "2025-11-15T17:20:53.521447Z",
          "iopub.status.idle": "2025-11-15T17:20:53.539825Z",
          "shell.execute_reply": "2025-11-15T17:20:53.539028Z",
          "shell.execute_reply.started": "2025-11-15T17:20:53.521747Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T17:20:53.541210Z",
          "iopub.status.busy": "2025-11-15T17:20:53.540661Z",
          "iopub.status.idle": "2025-11-15T17:20:53.556995Z",
          "shell.execute_reply": "2025-11-15T17:20:53.556426Z",
          "shell.execute_reply.started": "2025-11-15T17:20:53.541190Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience = 3,\n",
        "    early_stopping_threshold = 0.001\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T17:20:53.557914Z",
          "iopub.status.busy": "2025-11-15T17:20:53.557706Z",
          "iopub.status.idle": "2025-11-15T17:20:53.595419Z",
          "shell.execute_reply": "2025-11-15T17:20:53.594651Z",
          "shell.execute_reply.started": "2025-11-15T17:20:53.557898Z"
        },
        "id": "CSkCRB30QtLm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./vit-fake-detector_freezed\",\n",
        "    per_device_train_batch_size = BATCH_SIZE,\n",
        "    per_device_eval_batch_size = 8,\n",
        "    num_train_epochs = EPOCHS,\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate = LEARNING_RATE,\n",
        "    weight_decay = 0.01,\n",
        "    warmup_ratio = 0.1,\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = \"f1\",\n",
        "    greater_is_better = True,\n",
        "    logging_dir = './logs',\n",
        "    logging_steps = 100,\n",
        "    remove_unused_columns = False,\n",
        "    push_to_hub = False,\n",
        "    report_to = \"none\",\n",
        "    save_total_limit = 2,\n",
        "    dataloader_pin_memory = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T17:20:53.596688Z",
          "iopub.status.busy": "2025-11-15T17:20:53.596233Z",
          "iopub.status.idle": "2025-11-15T17:20:53.890034Z",
          "shell.execute_reply": "2025-11-15T17:20:53.889419Z",
          "shell.execute_reply.started": "2025-11-15T17:20:53.596659Z"
        },
        "id": "39g9QFKGR2Aq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer = MemoryEfficientTrainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    compute_metrics = compute_metrics,\n",
        "    callbacks = [early_stopping_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-15T17:20:53.891065Z",
          "iopub.status.busy": "2025-11-15T17:20:53.890836Z",
          "iopub.status.idle": "2025-11-15T17:20:54.259124Z",
          "shell.execute_reply": "2025-11-15T17:20:54.258470Z",
          "shell.execute_reply.started": "2025-11-15T17:20:53.891047Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEsp09tJR75b",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gj9rE-MR9Y8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f\"Test results: {test_results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for param in model.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-16T00:38:40.137249Z",
          "iopub.status.busy": "2025-11-16T00:38:40.136625Z",
          "iopub.status.idle": "2025-11-16T00:38:40.140671Z",
          "shell.execute_reply": "2025-11-16T00:38:40.139828Z",
          "shell.execute_reply.started": "2025-11-16T00:38:40.137226Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience = 3,\n",
        "    early_stopping_threshold = 0.001\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-16T00:38:31.003375Z",
          "iopub.status.busy": "2025-11-16T00:38:31.003081Z",
          "iopub.status.idle": "2025-11-16T00:38:31.034246Z",
          "shell.execute_reply": "2025-11-16T00:38:31.033474Z",
          "shell.execute_reply.started": "2025-11-16T00:38:31.003353Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./vit-fake-detector_unfreezed\",\n",
        "    per_device_train_batch_size = BATCH_SIZE,\n",
        "    per_device_eval_batch_size = 8,\n",
        "    num_train_epochs = EPOCHS,\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate = LEARNING_RATE,\n",
        "    weight_decay = 0.01,\n",
        "    warmup_ratio = 0.1,\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = \"f1\",\n",
        "    greater_is_better = True,\n",
        "    logging_dir = './logs',\n",
        "    logging_steps = 100,\n",
        "    remove_unused_columns = False,\n",
        "    push_to_hub = False,\n",
        "    report_to = \"none\",\n",
        "    save_total_limit = 2,\n",
        "    dataloader_pin_memory = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer = MemoryEfficientTrainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = val_dataset,\n",
        "    compute_metrics = compute_metrics,\n",
        "    callbacks = [early_stopping_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-16T00:38:48.719302Z",
          "iopub.status.busy": "2025-11-16T00:38:48.718659Z",
          "iopub.status.idle": "2025-11-16T00:42:52.944072Z",
          "shell.execute_reply": "2025-11-16T00:42:52.943417Z",
          "shell.execute_reply.started": "2025-11-16T00:38:48.719273Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(f\"Test results: {test_results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-16T01:04:22.032049Z",
          "iopub.status.busy": "2025-11-16T01:04:22.031471Z",
          "iopub.status.idle": "2025-11-16T01:07:14.479926Z",
          "shell.execute_reply": "2025-11-16T01:07:14.479060Z",
          "shell.execute_reply.started": "2025-11-16T01:04:22.032027Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "evaluate_per_domain(trainer, test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-16T00:42:52.945319Z",
          "iopub.status.busy": "2025-11-16T00:42:52.945049Z",
          "iopub.status.idle": "2025-11-16T00:42:52.948964Z",
          "shell.execute_reply": "2025-11-16T00:42:52.948189Z",
          "shell.execute_reply.started": "2025-11-16T00:42:52.945302Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "best_model = trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-16T00:45:44.176931Z",
          "iopub.status.busy": "2025-11-16T00:45:44.176722Z",
          "iopub.status.idle": "2025-11-16T00:45:49.083418Z",
          "shell.execute_reply": "2025-11-16T00:45:49.082324Z",
          "shell.execute_reply.started": "2025-11-16T00:45:44.176914Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n",
        "\n",
        "for idx in tmp:\n",
        "    image_path = test_dataset.file_paths[idx]\n",
        "    true_label = test_dataset.labels[idx]\n",
        "    visualize_attention(best_model, image_path, processor, true_label = true_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-16T00:45:49.084838Z",
          "iopub.status.busy": "2025-11-16T00:45:49.084479Z",
          "iopub.status.idle": "2025-11-16T00:45:55.465114Z",
          "shell.execute_reply": "2025-11-16T00:45:55.464407Z",
          "shell.execute_reply.started": "2025-11-16T00:45:49.084817Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n",
        "\n",
        "for idx in tmp:\n",
        "    image_path = test_dataset.file_paths[idx]\n",
        "    true_label = test_dataset.labels[idx]\n",
        "    visualize_attention(best_model, image_path, processor, true_label = true_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-16T00:45:55.466602Z",
          "iopub.status.busy": "2025-11-16T00:45:55.466133Z",
          "iopub.status.idle": "2025-11-16T00:46:00.349625Z",
          "shell.execute_reply": "2025-11-16T00:46:00.348937Z",
          "shell.execute_reply.started": "2025-11-16T00:45:55.466575Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tmp = np.random.randint(0, len(test_dataset.file_paths), 10)\n",
        "\n",
        "for idx in tmp:\n",
        "    image_path = test_dataset.file_paths[idx]\n",
        "    true_label = test_dataset.labels[idx]\n",
        "    visualize_attention(best_model, image_path, processor, true_label = true_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-16T00:53:11.097610Z",
          "iopub.status.busy": "2025-11-16T00:53:11.097315Z",
          "iopub.status.idle": "2025-11-16T00:53:47.311692Z",
          "shell.execute_reply": "2025-11-16T00:53:47.310806Z",
          "shell.execute_reply.started": "2025-11-16T00:53:11.097590Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "visualize_attention_per_domain(\n",
        "    model=best_model,  \n",
        "    test_dataset=test_dataset,\n",
        "    processor=processor,\n",
        "    n_samples=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hold-out Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-21T23:00:23.113251Z",
          "iopub.status.busy": "2025-11-21T23:00:23.112584Z",
          "iopub.status.idle": "2025-11-21T23:00:23.129606Z",
          "shell.execute_reply": "2025-11-21T23:00:23.128776Z",
          "shell.execute_reply.started": "2025-11-21T23:00:23.113224Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def evaluate_holdout_set(model, holdout_real_paths, holdout_fake_paths, \n",
        "                         processor, batch_size=32, model_name=\"ViT\"):\n",
        "    \"\"\"\n",
        "    Evaluate model on a new hold-out testing set (pure images, not from videos).\n",
        "    \n",
        "    Args:\n",
        "        model: Trained ViT model\n",
        "        holdout_real_paths: List of paths to real image folders\n",
        "        holdout_fake_paths: List of paths to fake image folders\n",
        "        processor: ViTImageProcessor\n",
        "        batch_size: Batch size for evaluation\n",
        "        model_name: Name of model for display\n",
        "    \n",
        "    Returns:\n",
        "        results: Dictionary with metrics and predictions\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(f\"HOLD-OUT SET EVALUATION - {model_name}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # LABEL_REAL = 0, LABEL_FAKE = 1\n",
        "    LABEL_REAL = 0\n",
        "    LABEL_FAKE = 1\n",
        "    \n",
        "    patterns_to_check = [\"*.png\", \"*.jpg\", \"*.jpeg\"]\n",
        "    \n",
        "    # Collect all real images\n",
        "    real_files = []\n",
        "    for path in holdout_real_paths:\n",
        "        for ext in patterns_to_check:\n",
        "            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n",
        "            real_files.extend(files)\n",
        "        print(f\"Found {len([f for f in real_files if path in f])} REAL images in {path}\")\n",
        "    \n",
        "    print(f\"Total REAL images: {len(real_files)}\")\n",
        "    \n",
        "    # Collect all fake images\n",
        "    fake_files = []\n",
        "    for path in holdout_fake_paths:\n",
        "        for ext in patterns_to_check:\n",
        "            files = glob.glob(os.path.join(path, \"**\", ext), recursive=True)\n",
        "            fake_files.extend(files)\n",
        "        print(f\"Found {len([f for f in fake_files if path in f])} FAKE images in {path}\")\n",
        "    \n",
        "    print(f\"Total FAKE images: {len(fake_files)}\")\n",
        "    \n",
        "    # Combine files and labels\n",
        "    all_files = real_files + fake_files\n",
        "    all_labels = [LABEL_REAL] * len(real_files) + [LABEL_FAKE] * len(fake_files)\n",
        "    \n",
        "    print(f\"\\nTotal hold-out images: {len(all_files)}\")\n",
        "    print(f\"  REAL: {len(real_files)}\")\n",
        "    print(f\"  FAKE: {len(fake_files)}\")\n",
        "    \n",
        "    if len(all_files) == 0:\n",
        "        print(\"Error: No images found!\")\n",
        "        return None\n",
        "    \n",
        "    # Create dataset\n",
        "    holdout_dataset = KaggleImageDataset(\n",
        "        file_paths=all_files,\n",
        "        labels=all_labels,\n",
        "        processor=processor,\n",
        "        is_train=False  # No augmentation for testing\n",
        "    )\n",
        "    \n",
        "    # Create dataloader\n",
        "    holdout_loader = DataLoader(\n",
        "        holdout_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "    \n",
        "    # Run inference\n",
        "    print(\"\\nRunning inference...\")\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    \n",
        "    all_preds = []\n",
        "    all_probs = []\n",
        "    all_labels_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(holdout_loader, desc=\"Evaluating\"):\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels_batch = batch['labels'].to(device)\n",
        "            \n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            \n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels_list.extend(labels_batch.cpu().numpy())\n",
        "    \n",
        "    all_preds = np.array(all_preds)\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels_array = np.array(all_labels_list)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    accuracy = accuracy_score(all_labels_array, all_preds)\n",
        "    print(f\"\\nOverall Accuracy: {accuracy*100:.2f}%\")\n",
        "    \n",
        "    # Classification report\n",
        "    print(\"\\n\" + classification_report(\n",
        "        all_labels_array, all_preds, \n",
        "        target_names=[\"REAL\", \"FAKE\"], \n",
        "        digits=4\n",
        "    ))\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels_array, all_preds)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(\"                 Predicted\")\n",
        "    print(\"               REAL    FAKE\")\n",
        "    print(f\"Actual REAL   {cm[0][0]:5d}   {cm[0][1]:5d}\")\n",
        "    print(f\"       FAKE   {cm[1][0]:5d}   {cm[1][1]:5d}\")\n",
        "    \n",
        "    # Calculate per-class metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels_array, all_preds, average=None, labels=[LABEL_REAL, LABEL_FAKE]\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nPer-Class Metrics:\")\n",
        "    print(f\"  REAL: Precision={precision[0]:.4f}, Recall={recall[0]:.4f}, F1={f1[0]:.4f}\")\n",
        "    print(f\"  FAKE: Precision={precision[1]:.4f}, Recall={recall[1]:.4f}, F1={f1[1]:.4f}\")\n",
        "    \n",
        "    # ROC-AUC\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels_array, all_probs[:, 1])\n",
        "        print(f\"\\nROC-AUC Score: {auc:.4f}\")\n",
        "    except:\n",
        "        print(\"\\nROC-AUC Score: Could not calculate\")\n",
        "        auc = None\n",
        "    \n",
        "    # Per-source breakdown\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PER-SOURCE BREAKDOWN\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Create source mapping\n",
        "    sources = {}\n",
        "    for path in holdout_real_paths:\n",
        "        source_name = os.path.basename(path.rstrip('/'))\n",
        "        sources[source_name] = {'pattern': path, 'type': 'REAL'}\n",
        "    \n",
        "    for path in holdout_fake_paths:\n",
        "        source_name = os.path.basename(path.rstrip('/'))\n",
        "        sources[source_name] = {'pattern': path, 'type': 'FAKE'}\n",
        "    \n",
        "    for source_name, source_info in sources.items():\n",
        "        # Find indices for this source\n",
        "        pattern = source_info['pattern']\n",
        "        indices = [i for i, f in enumerate(all_files) if pattern in f]\n",
        "        \n",
        "        if len(indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        source_labels = all_labels_array[indices]\n",
        "        source_preds = all_preds[indices]\n",
        "        \n",
        "        source_acc = accuracy_score(source_labels, source_preds)\n",
        "        correct = (source_labels == source_preds).sum()\n",
        "        \n",
        "        print(f\"\\n--- {source_name} ({source_info['type']}) ---\")\n",
        "        print(f\"Samples: {len(indices)}\")\n",
        "        print(f\"Accuracy: {source_acc*100:.2f}% ({correct}/{len(indices)})\")\n",
        "        \n",
        "        # Show breakdown if mixed labels\n",
        "        unique_labels = np.unique(source_labels)\n",
        "        if len(unique_labels) > 1:\n",
        "            source_report = classification_report(\n",
        "                source_labels, source_preds,\n",
        "                target_names=[\"REAL\", \"FAKE\"],\n",
        "                digits=4,\n",
        "                zero_division=0\n",
        "            )\n",
        "            print(source_report)\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Return results\n",
        "    results = {\n",
        "        'accuracy': accuracy,\n",
        "        'predictions': all_preds,\n",
        "        'probabilities': all_probs,\n",
        "        'labels': all_labels_array,\n",
        "        'file_paths': all_files,\n",
        "        'confusion_matrix': cm,\n",
        "        'auc': auc\n",
        "    }\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-21T23:00:48.586286Z",
          "iopub.status.busy": "2025-11-21T23:00:48.586011Z",
          "iopub.status.idle": "2025-11-21T23:00:48.949559Z",
          "shell.execute_reply": "2025-11-21T23:00:48.948883Z",
          "shell.execute_reply.started": "2025-11-21T23:00:48.586265Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "processor = AutoImageProcessor.from_pretrained(model_name, use_fast = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-21T23:00:50.614496Z",
          "iopub.status.busy": "2025-11-21T23:00:50.613763Z",
          "iopub.status.idle": "2025-11-21T23:00:51.446385Z",
          "shell.execute_reply": "2025-11-21T23:00:51.445215Z",
          "shell.execute_reply.started": "2025-11-21T23:00:50.614471Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "best_model = ViTForImageClassification.from_pretrained(\"/kaggle/input/vit-trained-model/checkpoint-17030\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-21T23:05:28.677957Z",
          "iopub.status.busy": "2025-11-21T23:05:28.677191Z",
          "iopub.status.idle": "2025-11-21T23:05:28.933668Z",
          "shell.execute_reply": "2025-11-21T23:05:28.932963Z",
          "shell.execute_reply.started": "2025-11-21T23:05:28.677911Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "best_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-21T23:05:35.616074Z",
          "iopub.status.busy": "2025-11-21T23:05:35.615412Z",
          "iopub.status.idle": "2025-11-21T23:05:35.621076Z",
          "shell.execute_reply": "2025-11-21T23:05:35.620268Z",
          "shell.execute_reply.started": "2025-11-21T23:05:35.616034Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "holdout_real_paths = [\n",
        "    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/test/real\",\n",
        "    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/train/real\",\n",
        "    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/val/real\"\n",
        "]\n",
        "\n",
        "holdout_fake_paths = [\n",
        "    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/test/fake\",\n",
        "    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/train/fake\",\n",
        "    \"/kaggle/input/deepfake-hold-out-testing/hold-out-testing/val/fake\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-21T23:05:36.459529Z",
          "iopub.status.busy": "2025-11-21T23:05:36.459257Z",
          "iopub.status.idle": "2025-11-21T23:10:43.038396Z",
          "shell.execute_reply": "2025-11-21T23:10:43.037480Z",
          "shell.execute_reply.started": "2025-11-21T23:05:36.459509Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "results = evaluate_holdout_set(\n",
        "    model = best_model,\n",
        "    holdout_real_paths = holdout_real_paths,\n",
        "    holdout_fake_paths = holdout_fake_paths,\n",
        "    processor = processor,\n",
        "    batch_size = 32,\n",
        "    model_name = \"ViT\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-21T23:11:39.070218Z",
          "iopub.status.busy": "2025-11-21T23:11:39.069744Z",
          "iopub.status.idle": "2025-11-21T23:11:39.074392Z",
          "shell.execute_reply": "2025-11-21T23:11:39.073727Z",
          "shell.execute_reply.started": "2025-11-21T23:11:39.070193Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if results:\n",
        "    print(f\"\\nFinal Accuracy: {results['accuracy']*100:.2f}%\")\n",
        "    if results['auc']:\n",
        "        print(f\"AUC: {results['auc']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 8734376,
          "sourceId": 13728437,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8706215,
          "sourceId": 13736632,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8724954,
          "sourceId": 13822102,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 8802348,
          "sourceId": 13822159,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31193,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "122561f392d14e3cafc6debb3187e24f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "628b1e6510cf46338d0e010fe095f2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb2707c882e94a97b32088ae4d492133",
            "max": 160,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f572c2a50ae64510bbf92c40ace30fc2",
            "value": 160
          }
        },
        "711124bc988c483182e547b57848b7dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9c22d583bb8412aa782e68c0c920c86",
              "IPY_MODEL_628b1e6510cf46338d0e010fe095f2c4",
              "IPY_MODEL_ff990731f4f645e2a8258dcecc41401a"
            ],
            "layout": "IPY_MODEL_b20e68a1589942b4820b320bfd0a7b58"
          }
        },
        "a9c22d583bb8412aa782e68c0c920c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_122561f392d14e3cafc6debb3187e24f",
            "placeholder": "​",
            "style": "IPY_MODEL_bfce42766f244ecd957c91c5ac56fd21",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "b20e68a1589942b4820b320bfd0a7b58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfce42766f244ecd957c91c5ac56fd21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9dd59964a914d649291782d647463c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ded7bcb289394a17a9ccb1f40e17fa0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb2707c882e94a97b32088ae4d492133": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f572c2a50ae64510bbf92c40ace30fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff990731f4f645e2a8258dcecc41401a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9dd59964a914d649291782d647463c3",
            "placeholder": "​",
            "style": "IPY_MODEL_ded7bcb289394a17a9ccb1f40e17fa0c",
            "value": " 160/160 [00:00&lt;00:00, 13.2kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
