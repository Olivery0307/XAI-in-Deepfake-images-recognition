{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepfake Detection Training Pipeline\n",
    "## Command Center for Google Colab Pro\n",
    "\n",
    "This notebook orchestrates the complete training pipeline:\n",
    "1. Environment setup (gcsfuse, dependencies)\n",
    "2. Data loading and splitting\n",
    "3. Model training with best checkpoint saving\n",
    "4. Evaluation and persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Cloud Storage with gcsfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gcsfuse\n",
    "!echo \"deb https://packages.cloud.google.com/apt gcsfuse-focal main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y gcsfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# Configure project\n",
    "PROJECT_ID = \"your-project-id\"  # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET_NAME = \"your-bucket-name\"  # REPLACE WITH YOUR BUCKET NAME\n",
    "\n",
    "!gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mount point and mount bucket\n",
    "!mkdir -p /content/gcs_data\n",
    "!gcsfuse --implicit-dirs {BUCKET_NAME} /content/gcs_data\n",
    "\n",
    "# Verify mount\n",
    "!ls -lh /content/gcs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import our custom modules\n",
    "from src.data_loader import get_data_mixed_structure, LocalImageDataset, save_splits, load_splits\n",
    "from src.preprocessing import get_transforms\n",
    "from src.models import get_model\n",
    "from src.trainer import main_training_loop, test_model\n",
    "from configs.config import Config\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "Config.create_directories()\n",
    "\n",
    "# Verify GCS mount\n",
    "if Config.validate_paths():\n",
    "    print(\"âœ“ GCS mount verified\")\n",
    "else:\n",
    "    print(\"âš  Warning: GCS mount point not found. Check your gcsfuse setup.\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(Config.SEED)\n",
    "\n",
    "# Get device\n",
    "device = Config.get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Display configuration\n",
    "print(\"\\nConfiguration:\")\n",
    "for key, value in Config.get_config_dict().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose model architecture\n",
    "MODEL_NAME = 'resnet34'  # Options: resnet34, resnet50, efficientnet_b0, efficientnet_b4, vit_b_16, vit_b_32\n",
    "\n",
    "# Determine model type for preprocessing\n",
    "MODEL_TYPE = 'vit' if 'vit' in MODEL_NAME else 'cnn'\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")\n",
    "print(f\"Model type: {MODEL_TYPE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Create new splits\n",
    "CREATE_NEW_SPLITS = True\n",
    "\n",
    "if CREATE_NEW_SPLITS:\n",
    "    print(\"Creating new data splits...\")\n",
    "    \n",
    "    train_data, val_data, test_data = get_data_mixed_structure(\n",
    "        celeb_real_path=Config.PATHS['celeb_real'],\n",
    "        youtube_real_path=Config.PATHS['youtube_real'],\n",
    "        celeb_synthesis_path=Config.PATHS['celeb_synthesis'],\n",
    "        ffhq_real_path=Config.PATHS['ffhq_real'],\n",
    "        stylegan_fake_path=Config.PATHS['stylegan_fake'],\n",
    "        stablediffusion_fake_path=Config.PATHS['stablediffusion_fake'],\n",
    "        train_ratio=Config.TRAIN_RATIO,\n",
    "        val_ratio=Config.VAL_RATIO,\n",
    "        test_ratio=Config.TEST_RATIO,\n",
    "        seed=Config.SEED,\n",
    "        # max_samples_per_category=1000  # Uncomment for quick debugging\n",
    "    )\n",
    "    \n",
    "    # Save splits for reproducibility\n",
    "    splits_path = os.path.join(Config.SPLITS_DIR, f'{MODEL_NAME}_splits.pkl')\n",
    "    save_splits(train_data, val_data, test_data, splits_path)\n",
    "    \n",
    "else:\n",
    "    # Option 2: Load existing splits\n",
    "    print(\"Loading existing splits...\")\n",
    "    splits_path = os.path.join(Config.SPLITS_DIR, f'{MODEL_NAME}_splits.pkl')\n",
    "    train_data, val_data, test_data = load_splits(splits_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transforms\n",
    "train_transform = get_transforms(split='train', model_type=MODEL_TYPE, img_size=Config.IMG_SIZE)\n",
    "val_transform = get_transforms(split='val', model_type=MODEL_TYPE, img_size=Config.IMG_SIZE)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = LocalImageDataset(train_data[0], train_data[1], transform=train_transform)\n",
    "val_dataset = LocalImageDataset(val_data[0], val_data[1], transform=val_transform)\n",
    "test_dataset = LocalImageDataset(test_data[0], test_data[1], transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=Config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=Config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=Config.NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches:   {len(val_loader)}\")\n",
    "print(f\"  Test batches:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Model, Loss, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = get_model(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_classes=Config.NUM_CLASSES,\n",
    "    pretrained=Config.PRETRAINED,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "\n",
    "# Learning rate scheduler (optional)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "print(\"\\nModel initialized and ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training loop\n",
    "history = main_training_loop(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=Config.NUM_EPOCHS,\n",
    "    device=device,\n",
    "    checkpoint_dir=Config.CHECKPOINT_DIR,\n",
    "    model_name=MODEL_NAME,\n",
    "    patience=Config.PATIENCE,\n",
    "    min_delta=Config.MIN_DELTA,\n",
    "    scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss and accuracy\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(history['train_acc'], label='Train Acc', marker='o')\n",
    "ax2.plot(history['val_acc'], label='Val Acc', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{Config.LOGS_DIR}/{MODEL_NAME}_training_history.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training plots saved to {Config.LOGS_DIR}/{MODEL_NAME}_training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load Best Model and Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import load_checkpoint\n",
    "\n",
    "# Load best model\n",
    "best_model_path = os.path.join(Config.CHECKPOINT_DIR, f'{MODEL_NAME}_best.pth')\n",
    "model = load_checkpoint(model, best_model_path, device=device)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = test_model(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nFinal Test Results:\")\n",
    "print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "print(f\"  Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Model for Streamlit App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in simple format for inference\n",
    "inference_model_path = f'{MODEL_NAME}.pth'\n",
    "torch.save(model.state_dict(), inference_model_path)\n",
    "\n",
    "print(f\"Model saved for inference: {inference_model_path}\")\n",
    "print(f\"Download this file to use with the Streamlit app!\")\n",
    "\n",
    "# Download to local machine (in Colab)\n",
    "from google.colab import files\n",
    "files.download(inference_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unmount GCS bucket\n",
    "!fusermount -u /content/gcs_data\n",
    "\n",
    "print(\"GCS bucket unmounted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Complete! ðŸŽ‰\n",
    "\n",
    "Next steps:\n",
    "1. Download the best model checkpoint\n",
    "2. Use it with the Streamlit app for inference\n",
    "3. Explore Grad-CAM visualizations in the app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
